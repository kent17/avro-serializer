{"Event":"SparkListenerLogStart","Spark Version":"1.4.1"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"192.168.2.68","Port":50438},"Maximum Memory":278019440,"Timestamp":1444384094784}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre","Java Version":"1.8.0_51 (Oracle Corporation)","Scala Version":"version 2.10.4"},"Spark Properties":{"spark.serializer":"org.apache.spark.serializer.KryoSerializer","spark.executor.extraJavaOptions":"-XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"","spark.driver.host":"192.168.2.68","spark.eventLog.enabled":"true","spark.driver.port":"50435","spark.jars":"file:/Users/quentin/Dev/avro-serializer/target/scala-2.10/sparkavroserializer_2.10-1.0.jar","spark.app.name":"SparkAvroSerializer","spark.scheduler.mode":"FIFO","spark.executor.id":"driver","spark.master":"spark://Quentins-MacBook-Pro.local:7077","spark.eventLog.dir":"spark-events","spark.fileserver.uri":"http://192.168.2.68:50436","spark.externalBlockStore.folderName":"spark-56f54831-cdd4-4e44-a547-a6f9c0c4e415","spark.app.id":"app-20151009114814-0002"},"System Properties":{"java.io.tmpdir":"/var/folders/hh/nlyyqy1j20x3xq4y7dflp6000000gn/T/","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/Users/quentin","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","ftp.nonProxyHosts":"local|*.local|169.254/16|*.169.254/16","sun.arch.data.model":"64","sun.boot.library.path":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib","user.dir":"/Users/quentin/Dev/avro-serializer","java.library.path":"/Users/quentin/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.","sun.cpu.isalist":"","os.arch":"x86_64","java.vm.version":"25.51-b03","java.endorsed.dirs":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/endorsed","java.runtime.version":"1.8.0_51-b16","java.vm.info":"mixed mode","java.ext.dirs":"/Users/quentin/Library/Java/Extensions:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/classes","file.encoding":"UTF-8","user.timezone":"Europe/Paris","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"10.10.5","sun.os.patch.level":"unknown","gopherProxySet":"false","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","http.nonProxyHosts":"local|*.local|169.254/16|*.169.254/16","user.language":"en","socksNonProxyHosts":"local|*.local|169.254/16|*.169.254/16","user.country.format":"FR","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.lwawt.macosx.CPrinterJob","java.awt.graphicsenv":"sun.awt.CGraphicsEnvironment","awt.toolkit":"sun.lwawt.macosx.LWCToolkit","os.name":"Mac OS X","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"quentin","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit --master spark://Quentins-MacBook-Pro.local:7077 --class com.avroserializer.SparkAvroExample target/scala-2.10/sparkavroserializer_2.10-1.0.jar","java.home":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre","java.version":"1.8.0_51","sun.io.unicode.encoding":"UnicodeBig"},"Classpath Entries":{"/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/spark-assembly-1.4.1-hadoop2.6.0.jar":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/conf/":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar":"System Classpath","http://192.168.2.68:50436/jars/sparkavroserializer_2.10-1.0.jar":"Added By User","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"SparkAvroSerializer","App ID":"app-20151009114814-0002","Timestamp":1444384092688,"User":"quentin"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1444384096590,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"4\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"4\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1444384099235,"Executor ID":"0","Executor Info":{"Host":"192.168.2.68","Total Cores":1,"Log Urls":{"stdout":"http://192.168.2.68:8082/logPage/?appId=app-20151009114814-0002&executorId=0&logType=stdout","stderr":"http://192.168.2.68:8082/logPage/?appId=app-20151009114814-0002&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1444384099239,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1444384099405,"Executor ID":"1","Executor Info":{"Host":"192.168.2.68","Total Cores":1,"Log Urls":{"stdout":"http://192.168.2.68:8081/logPage/?appId=app-20151009114814-0002&executorId=1&logType=stdout","stderr":"http://192.168.2.68:8081/logPage/?appId=app-20151009114814-0002&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.2.68","Port":50451},"Maximum Memory":278019440,"Timestamp":1444384099560}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.2.68","Port":50452},"Maximum Memory":278019440,"Timestamp":1444384099678}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1444384099239,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384102969,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":1849,"Executor Run Time":1448,"Result Size":1573,"JVM GC Time":187,"Result Serialization Time":2,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586770,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384097352,"Completion Time":1444384102973,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1444384102979,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1444384103166,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[1],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1444384103194,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1444384103194,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384105320,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":1331,"Executor Run Time":730,"Result Size":1754,"JVM GC Time":147,"Result Serialization Time":3,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384103193,"Completion Time":1444384105321,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1444384105321,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1444384105413,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[2],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1444384105431,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1444384105431,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384105944,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":77,"Executor Run Time":394,"Result Size":1753,"JVM GC Time":21,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384105431,"Completion Time":1444384105944,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1444384105944,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":3,"Submission Time":1444384106029,"Stage Infos":[{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[3],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":0,"Attempt":0,"Launch Time":1444384106042,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":3,"Index":0,"Attempt":0,"Launch Time":1444384106042,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384106215,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":48,"Executor Run Time":106,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384106042,"Completion Time":1444384106216,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":3,"Completion Time":1444384106216,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":4,"Submission Time":1444384106367,"Stage Infos":[{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[4],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"30\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384106388,"Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"30\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":4,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1444384106388,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":4,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1444384106388,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384106689,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":68,"Executor Run Time":205,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586754,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384106388,"Completion Time":1444384106689,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":4,"Completion Time":1444384106689,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":5,"Submission Time":1444384106772,"Stage Infos":[{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[5],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":5,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":0,"Attempt":0,"Launch Time":1444384106786,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":5,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":5,"Index":0,"Attempt":0,"Launch Time":1444384106786,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384106911,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":36,"Executor Run Time":72,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599050,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384106785,"Completion Time":1444384106912,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":5,"Completion Time":1444384106912,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":6,"Submission Time":1444384106969,"Stage Infos":[{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[6],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":6,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Launch Time":1444384106979,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":6,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Launch Time":1444384106979,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384107116,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":30,"Executor Run Time":95,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599050,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384106979,"Completion Time":1444384107117,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":6,"Completion Time":1444384107117,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":7,"Submission Time":1444384107174,"Stage Infos":[{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[7],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":7,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":0,"Attempt":0,"Launch Time":1444384107185,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":7,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":7,"Index":0,"Attempt":0,"Launch Time":1444384107185,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384107335,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":104,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599050,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384107185,"Completion Time":1444384107335,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":7,"Completion Time":1444384107335,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":8,"Submission Time":1444384107478,"Stage Infos":[{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[8],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"56\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"56\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":8,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1444384107503,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":8,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1444384107503,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384107790,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":70,"Executor Run Time":200,"Result Size":1573,"JVM GC Time":14,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586810,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384107503,"Completion Time":1444384107791,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":8,"Completion Time":1444384107791,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":9,"Submission Time":1444384107853,"Stage Infos":[{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[9],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":9,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Launch Time":1444384107864,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":9,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Launch Time":1444384107864,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384107971,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":63,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599106,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384107864,"Completion Time":1444384107972,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":9,"Completion Time":1444384107972,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":10,"Submission Time":1444384108082,"Stage Infos":[{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[10],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":10,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Launch Time":1444384108101,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":10,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Launch Time":1444384108101,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384108266,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":39,"Executor Run Time":115,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599106,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384108100,"Completion Time":1444384108267,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":10,"Completion Time":1444384108267,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":11,"Submission Time":1444384108326,"Stage Infos":[{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[11],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":11,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Launch Time":1444384108334,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":11,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Launch Time":1444384108334,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384108461,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":45,"Executor Run Time":70,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599106,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384108334,"Completion Time":1444384108462,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":11,"Completion Time":1444384108462,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":12,"Submission Time":1444384108559,"Stage Infos":[{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[12],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"82\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"82\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":12,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1444384108580,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":12,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1444384108580,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384108819,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":84,"Executor Run Time":141,"Result Size":1573,"JVM GC Time":19,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586773,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384108579,"Completion Time":1444384108819,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":12,"Completion Time":1444384108819,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":13,"Submission Time":1444384108875,"Stage Infos":[{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[13],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":13,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":0,"Attempt":0,"Launch Time":1444384108882,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":13,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":13,"Index":0,"Attempt":0,"Launch Time":1444384108882,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384109001,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":39,"Executor Run Time":69,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384108882,"Completion Time":1444384109002,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":13,"Completion Time":1444384109002,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":14,"Submission Time":1444384109064,"Stage Infos":[{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[14],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":14,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":0,"Attempt":0,"Launch Time":1444384109072,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":14,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":14,"Index":0,"Attempt":0,"Launch Time":1444384109072,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384109232,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":45,"Executor Run Time":104,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384109072,"Completion Time":1444384109233,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":14,"Completion Time":1444384109233,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":15,"Submission Time":1444384109285,"Stage Infos":[{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[15],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":15,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":0,"Attempt":0,"Launch Time":1444384109292,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":15,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":15,"Index":0,"Attempt":0,"Launch Time":1444384109292,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384109389,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":27,"Executor Run Time":61,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384109291,"Completion Time":1444384109390,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":15,"Completion Time":1444384109390,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":16,"Submission Time":1444384109444,"Stage Infos":[{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[16],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"108\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"108\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":16,"Stage Attempt ID":0,"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1444384109462,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":16,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1444384109462,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384109702,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":72,"Executor Run Time":153,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586813,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384109462,"Completion Time":1444384109702,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":16,"Completion Time":1444384109702,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":17,"Submission Time":1444384109757,"Stage Infos":[{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[17],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":17,"Stage Attempt ID":0,"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1444384109769,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":17,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1444384109769,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384109868,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":60,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599109,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384109768,"Completion Time":1444384109869,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":17,"Completion Time":1444384109869,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":18,"Submission Time":1444384109919,"Stage Infos":[{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[18],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":18,"Stage Attempt ID":0,"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1444384109927,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":18,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1444384109927,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384110152,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":33,"Executor Run Time":98,"Result Size":1753,"JVM GC Time":17,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599109,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384109926,"Completion Time":1444384110152,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":18,"Completion Time":1444384110153,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":19,"Submission Time":1444384110207,"Stage Infos":[{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[19],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":19,"Index":0,"Attempt":0,"Launch Time":1444384110213,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":19,"Index":0,"Attempt":0,"Launch Time":1444384110213,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384110322,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":80,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599109,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384110213,"Completion Time":1444384110323,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":19,"Completion Time":1444384110323,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":20,"Submission Time":1444384110501,"Stage Infos":[{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[20],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"134\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"134\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":20,"Stage Attempt ID":0,"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1444384110519,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":20,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1444384110519,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384111292,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":231,"Executor Run Time":514,"Result Size":1573,"JVM GC Time":22,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586770,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384110519,"Completion Time":1444384111292,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":20,"Completion Time":1444384111292,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":21,"Submission Time":1444384111366,"Stage Infos":[{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[21],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":21,"Stage Attempt ID":0,"Task Info":{"Task ID":21,"Index":0,"Attempt":0,"Launch Time":1444384111377,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":21,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":21,"Index":0,"Attempt":0,"Launch Time":1444384111377,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384111564,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":55,"Executor Run Time":67,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384111376,"Completion Time":1444384111565,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":21,"Completion Time":1444384111565,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":22,"Submission Time":1444384111618,"Stage Infos":[{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[22],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":22,"Stage Attempt ID":0,"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1444384111634,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":22,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1444384111634,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384111726,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":61,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384111633,"Completion Time":1444384111727,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":22,"Completion Time":1444384111727,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":23,"Submission Time":1444384111784,"Stage Infos":[{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[23],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":23,"Stage Attempt ID":0,"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Launch Time":1444384111792,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":23,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Launch Time":1444384111792,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384111911,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":21,"Executor Run Time":89,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384111791,"Completion Time":1444384111912,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":23,"Completion Time":1444384111912,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":24,"Submission Time":1444384111961,"Stage Infos":[{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[24],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"160\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"160\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1444384111976,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1444384111976,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384112127,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":41,"Executor Run Time":99,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586753,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384111976,"Completion Time":1444384112128,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":24,"Completion Time":1444384112128,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":25,"Submission Time":1444384112180,"Stage Infos":[{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[25],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":25,"Stage Attempt ID":0,"Task Info":{"Task ID":25,"Index":0,"Attempt":0,"Launch Time":1444384112187,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":25,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":25,"Index":0,"Attempt":0,"Launch Time":1444384112187,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384112274,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":59,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599049,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384112187,"Completion Time":1444384112275,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":25,"Completion Time":1444384112275,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":26,"Submission Time":1444384112322,"Stage Infos":[{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[26],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":26,"Stage Attempt ID":0,"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1444384112328,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":26,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1444384112328,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384112413,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":53,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599049,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384112328,"Completion Time":1444384112414,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":26,"Completion Time":1444384112415,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":27,"Submission Time":1444384112462,"Stage Infos":[{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[27],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":27,"Stage Attempt ID":0,"Task Info":{"Task ID":27,"Index":0,"Attempt":0,"Launch Time":1444384112468,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":27,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":27,"Index":0,"Attempt":0,"Launch Time":1444384112468,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384112569,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":70,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599049,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384112468,"Completion Time":1444384112570,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":27,"Completion Time":1444384112570,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":28,"Submission Time":1444384112632,"Stage Infos":[{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[28],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"186\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"186\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":28,"Stage Attempt ID":0,"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1444384112649,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":28,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1444384112649,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384113159,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":100,"Executor Run Time":389,"Result Size":1573,"JVM GC Time":17,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586792,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384112648,"Completion Time":1444384113159,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":28,"Completion Time":1444384113159,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":29,"Submission Time":1444384113222,"Stage Infos":[{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[29],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":29,"Stage Attempt ID":0,"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Launch Time":1444384113227,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":29,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Launch Time":1444384113227,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384113347,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":88,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599088,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384113227,"Completion Time":1444384113347,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":29,"Completion Time":1444384113347,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":30,"Submission Time":1444384113470,"Stage Infos":[{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[30],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":30,"Stage Attempt ID":0,"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1444384113485,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":30,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1444384113485,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384113638,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":34,"Executor Run Time":108,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599088,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384113485,"Completion Time":1444384113639,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":30,"Completion Time":1444384113639,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":31,"Submission Time":1444384113682,"Stage Infos":[{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[31],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":31,"Stage Attempt ID":0,"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1444384113690,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":31,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1444384113690,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384113804,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":20,"Executor Run Time":81,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599088,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384113689,"Completion Time":1444384113806,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":31,"Completion Time":1444384113807,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":32,"Submission Time":1444384113950,"Stage Infos":[{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[32],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"212\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"212\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":32,"Stage Attempt ID":0,"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1444384113966,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":32,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1444384113966,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384114168,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":53,"Executor Run Time":136,"Result Size":1573,"JVM GC Time":10,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586774,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384113965,"Completion Time":1444384114169,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":32,"Completion Time":1444384114169,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":33,"Submission Time":1444384114221,"Stage Infos":[{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[33],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":33,"Stage Attempt ID":0,"Task Info":{"Task ID":33,"Index":0,"Attempt":0,"Launch Time":1444384114228,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":33,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":33,"Index":0,"Attempt":0,"Launch Time":1444384114228,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384114330,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":25,"Executor Run Time":61,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599070,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384114228,"Completion Time":1444384114330,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":33,"Completion Time":1444384114330,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":34,"Submission Time":1444384114376,"Stage Infos":[{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[34],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":34,"Stage Attempt ID":0,"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1444384114382,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":34,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1444384114382,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384114473,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":37,"Executor Run Time":46,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599070,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384114381,"Completion Time":1444384114474,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":34,"Completion Time":1444384114474,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":35,"Submission Time":1444384114520,"Stage Infos":[{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[35],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":35,"Stage Attempt ID":0,"Task Info":{"Task ID":35,"Index":0,"Attempt":0,"Launch Time":1444384114527,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":35,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":35,"Index":0,"Attempt":0,"Launch Time":1444384114527,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384114618,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":27,"Executor Run Time":49,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599070,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384114526,"Completion Time":1444384114618,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":35,"Completion Time":1444384114618,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":36,"Submission Time":1444384114671,"Stage Infos":[{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[36],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"238\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"238\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":36,"Stage Attempt ID":0,"Task Info":{"Task ID":36,"Index":0,"Attempt":0,"Launch Time":1444384114690,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":36,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":36,"Index":0,"Attempt":0,"Launch Time":1444384114690,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384114956,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":100,"Executor Run Time":154,"Result Size":1573,"JVM GC Time":8,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586798,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384114689,"Completion Time":1444384114956,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":36,"Completion Time":1444384114956,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":37,"Submission Time":1444384115066,"Stage Infos":[{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[37],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":37,"Stage Attempt ID":0,"Task Info":{"Task ID":37,"Index":0,"Attempt":0,"Launch Time":1444384115073,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":37,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":37,"Index":0,"Attempt":0,"Launch Time":1444384115073,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384115207,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":101,"Result Size":1754,"JVM GC Time":20,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599094,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384115073,"Completion Time":1444384115215,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":37,"Completion Time":1444384115215,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":38,"Submission Time":1444384115284,"Stage Infos":[{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[38],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":38,"Stage Attempt ID":0,"Task Info":{"Task ID":38,"Index":0,"Attempt":0,"Launch Time":1444384115292,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":38,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":38,"Index":0,"Attempt":0,"Launch Time":1444384115292,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384115424,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":37,"Executor Run Time":85,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599094,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384115292,"Completion Time":1444384115425,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":38,"Completion Time":1444384115425,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":39,"Submission Time":1444384115482,"Stage Infos":[{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[39],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":39,"Stage Attempt ID":0,"Task Info":{"Task ID":39,"Index":0,"Attempt":0,"Launch Time":1444384115493,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":39,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":39,"Index":0,"Attempt":0,"Launch Time":1444384115493,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384115679,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":64,"Executor Run Time":103,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599094,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384115492,"Completion Time":1444384115679,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":39,"Completion Time":1444384115679,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":40,"Submission Time":1444384115775,"Stage Infos":[{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[40],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"264\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"264\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":40,"Stage Attempt ID":0,"Task Info":{"Task ID":40,"Index":0,"Attempt":0,"Launch Time":1444384115793,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":40,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":40,"Index":0,"Attempt":0,"Launch Time":1444384115793,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384116134,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":70,"Executor Run Time":244,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586750,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384115793,"Completion Time":1444384116134,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":40,"Completion Time":1444384116134,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":41,"Submission Time":1444384116189,"Stage Infos":[{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[41],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":41,"Stage Attempt ID":0,"Task Info":{"Task ID":41,"Index":0,"Attempt":0,"Launch Time":1444384116195,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":41,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":41,"Index":0,"Attempt":0,"Launch Time":1444384116195,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384116383,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":121,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599046,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384116195,"Completion Time":1444384116384,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":41,"Completion Time":1444384116384,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":42,"Submission Time":1444384116436,"Stage Infos":[{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[42],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":42,"Stage Attempt ID":0,"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Launch Time":1444384116444,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":42,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Launch Time":1444384116444,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384116567,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":84,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599046,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384116444,"Completion Time":1444384116568,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":42,"Completion Time":1444384116568,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":43,"Submission Time":1444384116614,"Stage Infos":[{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[43],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":43,"Stage Attempt ID":0,"Task Info":{"Task ID":43,"Index":0,"Attempt":0,"Launch Time":1444384116620,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":43,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":43,"Index":0,"Attempt":0,"Launch Time":1444384116620,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384116715,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":20,"Executor Run Time":61,"Result Size":1754,"JVM GC Time":9,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599046,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384116620,"Completion Time":1444384116715,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":43,"Completion Time":1444384116715,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":44,"Submission Time":1444384116835,"Stage Infos":[{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[44],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"290\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"290\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":44,"Stage Attempt ID":0,"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Launch Time":1444384116857,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":44,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Launch Time":1444384116857,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384117160,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":60,"Executor Run Time":227,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586773,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384116857,"Completion Time":1444384117160,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":44,"Completion Time":1444384117161,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":45,"Submission Time":1444384117205,"Stage Infos":[{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[45],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":45,"Stage Attempt ID":0,"Task Info":{"Task ID":45,"Index":0,"Attempt":0,"Launch Time":1444384117210,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":45,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":45,"Index":0,"Attempt":0,"Launch Time":1444384117210,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384117275,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":43,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384117210,"Completion Time":1444384117275,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":45,"Completion Time":1444384117275,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":46,"Submission Time":1444384117317,"Stage Infos":[{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[46],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":46,"Stage Attempt ID":0,"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Launch Time":1444384117322,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":46,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Launch Time":1444384117322,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384117396,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":43,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384117322,"Completion Time":1444384117396,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":46,"Completion Time":1444384117396,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":47,"Submission Time":1444384117440,"Stage Infos":[{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[47],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":47,"Stage Attempt ID":0,"Task Info":{"Task ID":47,"Index":0,"Attempt":0,"Launch Time":1444384117445,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":47,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":47,"Index":0,"Attempt":0,"Launch Time":1444384117445,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384117520,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":43,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384117445,"Completion Time":1444384117520,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":47,"Completion Time":1444384117520,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":48,"Submission Time":1444384117564,"Stage Infos":[{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[48],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"316\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"316\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":48,"Stage Attempt ID":0,"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Launch Time":1444384117581,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":48,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Launch Time":1444384117581,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384117735,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":41,"Executor Run Time":99,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586806,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384117580,"Completion Time":1444384117736,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":48,"Completion Time":1444384117736,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":49,"Submission Time":1444384117782,"Stage Infos":[{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[49],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":49,"Stage Attempt ID":0,"Task Info":{"Task ID":49,"Index":0,"Attempt":0,"Launch Time":1444384117786,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":49,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":49,"Index":0,"Attempt":0,"Launch Time":1444384117786,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384117848,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":41,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599102,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384117786,"Completion Time":1444384117849,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":49,"Completion Time":1444384117849,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":50,"Submission Time":1444384117894,"Stage Infos":[{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[50],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":50,"Stage Attempt ID":0,"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Launch Time":1444384117903,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":50,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Launch Time":1444384117903,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384117994,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":68,"Result Size":1753,"JVM GC Time":16,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599102,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384117903,"Completion Time":1444384117994,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":50,"Completion Time":1444384117994,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":51,"Submission Time":1444384118065,"Stage Infos":[{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[51],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":51,"Stage Attempt ID":0,"Task Info":{"Task ID":51,"Index":0,"Attempt":0,"Launch Time":1444384118069,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":51,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":51,"Index":0,"Attempt":0,"Launch Time":1444384118069,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384118285,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":88,"Executor Run Time":115,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599102,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384118069,"Completion Time":1444384118285,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":51,"Completion Time":1444384118285,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":52,"Submission Time":1444384118340,"Stage Infos":[{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[52],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"342\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"342\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":52,"Stage Attempt ID":0,"Task Info":{"Task ID":52,"Index":0,"Attempt":0,"Launch Time":1444384118355,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":52,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":52,"Index":0,"Attempt":0,"Launch Time":1444384118355,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384118685,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":57,"Executor Run Time":261,"Result Size":1573,"JVM GC Time":20,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586790,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384118354,"Completion Time":1444384118685,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":52,"Completion Time":1444384118685,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":53,"Submission Time":1444384118731,"Stage Infos":[{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[53],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":53,"Stage Attempt ID":0,"Task Info":{"Task ID":53,"Index":0,"Attempt":0,"Launch Time":1444384118736,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":53,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":53,"Index":0,"Attempt":0,"Launch Time":1444384118736,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384118826,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":64,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384118736,"Completion Time":1444384118826,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":53,"Completion Time":1444384118826,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":54,"Submission Time":1444384118877,"Stage Infos":[{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[54],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":54,"Stage Attempt ID":0,"Task Info":{"Task ID":54,"Index":0,"Attempt":0,"Launch Time":1444384118882,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":54,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":54,"Index":0,"Attempt":0,"Launch Time":1444384118882,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384118976,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":59,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384118882,"Completion Time":1444384118976,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":54,"Completion Time":1444384118976,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":55,"Submission Time":1444384119021,"Stage Infos":[{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[55],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":55,"Stage Attempt ID":0,"Task Info":{"Task ID":55,"Index":0,"Attempt":0,"Launch Time":1444384119029,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":55,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":55,"Index":0,"Attempt":0,"Launch Time":1444384119029,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384119144,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":90,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384119028,"Completion Time":1444384119144,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":55,"Completion Time":1444384119144,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":56,"Submission Time":1444384119222,"Stage Infos":[{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[56],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"368\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"368\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":56,"Stage Attempt ID":0,"Task Info":{"Task ID":56,"Index":0,"Attempt":0,"Launch Time":1444384119243,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":56,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":56,"Index":0,"Attempt":0,"Launch Time":1444384119243,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384119603,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":109,"Executor Run Time":223,"Result Size":1573,"JVM GC Time":7,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586741,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384119243,"Completion Time":1444384119604,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":56,"Completion Time":1444384119604,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":57,"Submission Time":1444384119657,"Stage Infos":[{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[57],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":57,"Stage Attempt ID":0,"Task Info":{"Task ID":57,"Index":0,"Attempt":0,"Launch Time":1444384119663,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":57,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":57,"Index":0,"Attempt":0,"Launch Time":1444384119663,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384119778,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":41,"Executor Run Time":58,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599037,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384119662,"Completion Time":1444384119779,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":57,"Completion Time":1444384119779,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":58,"Submission Time":1444384119821,"Stage Infos":[{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[58],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":58,"Stage Attempt ID":0,"Task Info":{"Task ID":58,"Index":0,"Attempt":0,"Launch Time":1444384119853,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":58,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":58,"Index":0,"Attempt":0,"Launch Time":1444384119853,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384119986,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":84,"Executor Run Time":41,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599037,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384119853,"Completion Time":1444384119986,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":58,"Completion Time":1444384119986,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":59,"Submission Time":1444384120028,"Stage Infos":[{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[59],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":59,"Stage Attempt ID":0,"Task Info":{"Task ID":59,"Index":0,"Attempt":0,"Launch Time":1444384120033,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":59,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":59,"Index":0,"Attempt":0,"Launch Time":1444384120033,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384120103,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":44,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599037,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384120032,"Completion Time":1444384120103,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":59,"Completion Time":1444384120103,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":60,"Submission Time":1444384120182,"Stage Infos":[{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[60],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"394\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"394\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":60,"Stage Attempt ID":0,"Task Info":{"Task ID":60,"Index":0,"Attempt":0,"Launch Time":1444384120205,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":60,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":60,"Index":0,"Attempt":0,"Launch Time":1444384120205,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384120437,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":46,"Executor Run Time":174,"Result Size":1573,"JVM GC Time":7,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586754,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384120205,"Completion Time":1444384120437,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":60,"Completion Time":1444384120437,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":61,"Submission Time":1444384120483,"Stage Infos":[{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[61],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":61,"Stage Attempt ID":0,"Task Info":{"Task ID":61,"Index":0,"Attempt":0,"Launch Time":1444384120489,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":61,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":61,"Index":0,"Attempt":0,"Launch Time":1444384120489,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384120568,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":54,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599050,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384120489,"Completion Time":1444384120569,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":61,"Completion Time":1444384120569,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":62,"Submission Time":1444384120626,"Stage Infos":[{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[62],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":62,"Stage Attempt ID":0,"Task Info":{"Task ID":62,"Index":0,"Attempt":0,"Launch Time":1444384120630,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":62,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":62,"Index":0,"Attempt":0,"Launch Time":1444384120630,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384120702,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":50,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599050,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384120630,"Completion Time":1444384120703,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":62,"Completion Time":1444384120703,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":63,"Submission Time":1444384120746,"Stage Infos":[{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[63],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":63,"Stage Attempt ID":0,"Task Info":{"Task ID":63,"Index":0,"Attempt":0,"Launch Time":1444384120751,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":63,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":63,"Index":0,"Attempt":0,"Launch Time":1444384120751,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384120814,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":40,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599050,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384120751,"Completion Time":1444384120814,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":63,"Completion Time":1444384120815,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":64,"Submission Time":1444384120866,"Stage Infos":[{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[64],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"420\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"420\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":64,"Stage Attempt ID":0,"Task Info":{"Task ID":64,"Index":0,"Attempt":0,"Launch Time":1444384120880,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":64,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":64,"Index":0,"Attempt":0,"Launch Time":1444384120880,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384121075,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":51,"Executor Run Time":136,"Result Size":1573,"JVM GC Time":6,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586800,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384120880,"Completion Time":1444384121076,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":64,"Completion Time":1444384121076,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":65,"Submission Time":1444384121120,"Stage Infos":[{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[65],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":65,"Stage Attempt ID":0,"Task Info":{"Task ID":65,"Index":0,"Attempt":0,"Launch Time":1444384121124,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":65,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":65,"Index":0,"Attempt":0,"Launch Time":1444384121124,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384121197,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":52,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384121124,"Completion Time":1444384121197,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":65,"Completion Time":1444384121197,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":66,"Submission Time":1444384121297,"Stage Infos":[{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[66],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":66,"Stage Attempt ID":0,"Task Info":{"Task ID":66,"Index":0,"Attempt":0,"Launch Time":1444384121306,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":66,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":66,"Index":0,"Attempt":0,"Launch Time":1444384121306,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384121393,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":37,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384121306,"Completion Time":1444384121393,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":66,"Completion Time":1444384121393,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":67,"Submission Time":1444384121438,"Stage Infos":[{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[67],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":67,"Stage Attempt ID":0,"Task Info":{"Task ID":67,"Index":0,"Attempt":0,"Launch Time":1444384121444,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":67,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":67,"Index":0,"Attempt":0,"Launch Time":1444384121444,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384121501,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384121443,"Completion Time":1444384121501,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":67,"Completion Time":1444384121501,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":68,"Submission Time":1444384121546,"Stage Infos":[{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[68],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"446\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"446\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":68,"Stage Attempt ID":0,"Task Info":{"Task ID":68,"Index":0,"Attempt":0,"Launch Time":1444384121561,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":68,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":68,"Index":0,"Attempt":0,"Launch Time":1444384121561,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384121746,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":36,"Executor Run Time":140,"Result Size":1573,"JVM GC Time":10,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586782,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384121561,"Completion Time":1444384121747,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":68,"Completion Time":1444384121747,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":69,"Submission Time":1444384121794,"Stage Infos":[{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[69],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":69,"Stage Attempt ID":0,"Task Info":{"Task ID":69,"Index":0,"Attempt":0,"Launch Time":1444384121799,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":69,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":69,"Index":0,"Attempt":0,"Launch Time":1444384121799,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384121851,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":11,"Executor Run Time":35,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384121798,"Completion Time":1444384121851,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":69,"Completion Time":1444384121851,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":70,"Submission Time":1444384121889,"Stage Infos":[{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[70],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":70,"Stage Attempt ID":0,"Task Info":{"Task ID":70,"Index":0,"Attempt":0,"Launch Time":1444384121893,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":70,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":70,"Index":0,"Attempt":0,"Launch Time":1444384121893,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384121954,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":36,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384121893,"Completion Time":1444384121956,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":70,"Completion Time":1444384121956,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":71,"Submission Time":1444384121999,"Stage Infos":[{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[71],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":71,"Stage Attempt ID":0,"Task Info":{"Task ID":71,"Index":0,"Attempt":0,"Launch Time":1444384122005,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":71,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":71,"Index":0,"Attempt":0,"Launch Time":1444384122005,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384122062,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384122005,"Completion Time":1444384122063,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":71,"Completion Time":1444384122063,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":72,"Submission Time":1444384122125,"Stage Infos":[{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[72],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"472\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"472\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":72,"Stage Attempt ID":0,"Task Info":{"Task ID":72,"Index":0,"Attempt":0,"Launch Time":1444384122143,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":72,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":72,"Index":0,"Attempt":0,"Launch Time":1444384122143,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384122373,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":43,"Executor Run Time":173,"Result Size":1573,"JVM GC Time":20,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586772,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384122143,"Completion Time":1444384122373,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":72,"Completion Time":1444384122373,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":73,"Submission Time":1444384122439,"Stage Infos":[{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[73],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":73,"Stage Attempt ID":0,"Task Info":{"Task ID":73,"Index":0,"Attempt":0,"Launch Time":1444384122469,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":73,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":73,"Index":0,"Attempt":0,"Launch Time":1444384122469,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384122579,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":43,"Executor Run Time":60,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599068,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384122469,"Completion Time":1444384122580,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":73,"Completion Time":1444384122580,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":74,"Submission Time":1444384122623,"Stage Infos":[{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[74],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":74,"Stage Attempt ID":0,"Task Info":{"Task ID":74,"Index":0,"Attempt":0,"Launch Time":1444384122630,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":74,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":74,"Index":0,"Attempt":0,"Launch Time":1444384122630,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384122689,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":37,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599068,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384122630,"Completion Time":1444384122690,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":74,"Completion Time":1444384122690,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":75,"Submission Time":1444384122736,"Stage Infos":[{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[75],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":75,"Stage Attempt ID":0,"Task Info":{"Task ID":75,"Index":0,"Attempt":0,"Launch Time":1444384122741,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":75,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":75,"Index":0,"Attempt":0,"Launch Time":1444384122741,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384122800,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":37,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599068,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384122741,"Completion Time":1444384122800,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":75,"Completion Time":1444384122801,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":76,"Submission Time":1444384122846,"Stage Infos":[{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[76],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"498\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"498\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":76,"Stage Attempt ID":0,"Task Info":{"Task ID":76,"Index":0,"Attempt":0,"Launch Time":1444384122858,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":76,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":76,"Index":0,"Attempt":0,"Launch Time":1444384122858,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384123043,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":32,"Executor Run Time":145,"Result Size":1573,"JVM GC Time":16,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586791,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384122858,"Completion Time":1444384123044,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":76,"Completion Time":1444384123044,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":77,"Submission Time":1444384123089,"Stage Infos":[{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[77],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":77,"Stage Attempt ID":0,"Task Info":{"Task ID":77,"Index":0,"Attempt":0,"Launch Time":1444384123094,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":77,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":77,"Index":0,"Attempt":0,"Launch Time":1444384123094,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384123155,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":42,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599087,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384123094,"Completion Time":1444384123156,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":77,"Completion Time":1444384123156,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":78,"Submission Time":1444384123197,"Stage Infos":[{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[78],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":78,"Stage Attempt ID":0,"Task Info":{"Task ID":78,"Index":0,"Attempt":0,"Launch Time":1444384123202,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":78,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":78,"Index":0,"Attempt":0,"Launch Time":1444384123202,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384123273,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":46,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599087,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384123202,"Completion Time":1444384123275,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":78,"Completion Time":1444384123275,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":79,"Submission Time":1444384123319,"Stage Infos":[{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[79],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":79,"Stage Attempt ID":0,"Task Info":{"Task ID":79,"Index":0,"Attempt":0,"Launch Time":1444384123325,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":79,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":79,"Index":0,"Attempt":0,"Launch Time":1444384123325,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384123402,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":59,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599087,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384123325,"Completion Time":1444384123403,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":79,"Completion Time":1444384123403,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":80,"Submission Time":1444384123443,"Stage Infos":[{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[80],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"524\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"524\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":80,"Stage Attempt ID":0,"Task Info":{"Task ID":80,"Index":0,"Attempt":0,"Launch Time":1444384123455,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":80,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":80,"Index":0,"Attempt":0,"Launch Time":1444384123455,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384123615,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":32,"Executor Run Time":118,"Result Size":1573,"JVM GC Time":20,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586760,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384123455,"Completion Time":1444384123615,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":80,"Completion Time":1444384123615,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":81,"Submission Time":1444384123741,"Stage Infos":[{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[81],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":81,"Stage Attempt ID":0,"Task Info":{"Task ID":81,"Index":0,"Attempt":0,"Launch Time":1444384123749,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":81,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":81,"Index":0,"Attempt":0,"Launch Time":1444384123749,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384123807,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":36,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599056,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384123748,"Completion Time":1444384123808,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":81,"Completion Time":1444384123808,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":82,"Submission Time":1444384123858,"Stage Infos":[{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[82],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":82,"Stage Attempt ID":0,"Task Info":{"Task ID":82,"Index":0,"Attempt":0,"Launch Time":1444384123862,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":82,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":82,"Index":0,"Attempt":0,"Launch Time":1444384123862,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384123935,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":53,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599056,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384123862,"Completion Time":1444384123935,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":82,"Completion Time":1444384123936,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":83,"Submission Time":1444384123983,"Stage Infos":[{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[83],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":83,"Stage Attempt ID":0,"Task Info":{"Task ID":83,"Index":0,"Attempt":0,"Launch Time":1444384123987,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":83,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":83,"Index":0,"Attempt":0,"Launch Time":1444384123987,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384124048,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599056,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384123987,"Completion Time":1444384124049,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":83,"Completion Time":1444384124049,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":84,"Submission Time":1444384124102,"Stage Infos":[{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[84],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"550\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"550\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":84,"Stage Attempt ID":0,"Task Info":{"Task ID":84,"Index":0,"Attempt":0,"Launch Time":1444384124118,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":84,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":84,"Index":0,"Attempt":0,"Launch Time":1444384124118,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384124330,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":38,"Executor Run Time":165,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586757,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384124118,"Completion Time":1444384124331,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":84,"Completion Time":1444384124331,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":85,"Submission Time":1444384124383,"Stage Infos":[{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[85],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":85,"Stage Attempt ID":0,"Task Info":{"Task ID":85,"Index":0,"Attempt":0,"Launch Time":1444384124388,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":85,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":85,"Index":0,"Attempt":0,"Launch Time":1444384124388,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384124518,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":102,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599053,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384124388,"Completion Time":1444384124519,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":85,"Completion Time":1444384124519,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":86,"Submission Time":1444384124563,"Stage Infos":[{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[86],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":86,"Stage Attempt ID":0,"Task Info":{"Task ID":86,"Index":0,"Attempt":0,"Launch Time":1444384124568,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":86,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":86,"Index":0,"Attempt":0,"Launch Time":1444384124568,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384124627,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":38,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599053,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384124568,"Completion Time":1444384124628,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":86,"Completion Time":1444384124628,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":87,"Submission Time":1444384124672,"Stage Infos":[{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[87],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":87,"Stage Attempt ID":0,"Task Info":{"Task ID":87,"Index":0,"Attempt":0,"Launch Time":1444384124678,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":87,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":87,"Index":0,"Attempt":0,"Launch Time":1444384124678,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384124738,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":40,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599053,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384124678,"Completion Time":1444384124739,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":87,"Completion Time":1444384124739,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":88,"Submission Time":1444384124794,"Stage Infos":[{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[88],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"576\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"576\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":88,"Stage Attempt ID":0,"Task Info":{"Task ID":88,"Index":0,"Attempt":0,"Launch Time":1444384124833,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":88,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":88,"Index":0,"Attempt":0,"Launch Time":1444384124833,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384125061,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":71,"Executor Run Time":148,"Result Size":1573,"JVM GC Time":21,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586775,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384124833,"Completion Time":1444384125063,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":88,"Completion Time":1444384125063,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":89,"Submission Time":1444384125110,"Stage Infos":[{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[89],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":89,"Stage Attempt ID":0,"Task Info":{"Task ID":89,"Index":0,"Attempt":0,"Launch Time":1444384125114,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":89,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":89,"Index":0,"Attempt":0,"Launch Time":1444384125114,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384125194,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":55,"Result Size":1754,"JVM GC Time":17,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599071,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384125113,"Completion Time":1444384125194,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":89,"Completion Time":1444384125194,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":90,"Submission Time":1444384125237,"Stage Infos":[{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[90],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":90,"Stage Attempt ID":0,"Task Info":{"Task ID":90,"Index":0,"Attempt":0,"Launch Time":1444384125242,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":90,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":90,"Index":0,"Attempt":0,"Launch Time":1444384125242,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384125296,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":11,"Executor Run Time":36,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599071,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384125241,"Completion Time":1444384125296,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":90,"Completion Time":1444384125296,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":91,"Submission Time":1444384125341,"Stage Infos":[{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[91],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":91,"Stage Attempt ID":0,"Task Info":{"Task ID":91,"Index":0,"Attempt":0,"Launch Time":1444384125346,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":91,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":91,"Index":0,"Attempt":0,"Launch Time":1444384125346,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384125415,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":11,"Executor Run Time":40,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599071,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384125345,"Completion Time":1444384125415,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":91,"Completion Time":1444384125415,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":92,"Submission Time":1444384125482,"Stage Infos":[{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[92],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"602\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"602\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":92,"Stage Attempt ID":0,"Task Info":{"Task ID":92,"Index":0,"Attempt":0,"Launch Time":1444384125504,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":92,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":92,"Index":0,"Attempt":0,"Launch Time":1444384125504,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384125740,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":47,"Executor Run Time":179,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586799,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384125504,"Completion Time":1444384125740,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":92,"Completion Time":1444384125740,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":93,"Submission Time":1444384125798,"Stage Infos":[{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[93],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":93,"Stage Attempt ID":0,"Task Info":{"Task ID":93,"Index":0,"Attempt":0,"Launch Time":1444384125808,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":93,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":93,"Index":0,"Attempt":0,"Launch Time":1444384125808,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384125891,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":58,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599095,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384125807,"Completion Time":1444384125891,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":93,"Completion Time":1444384125892,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":94,"Submission Time":1444384125943,"Stage Infos":[{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[94],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":94,"Stage Attempt ID":0,"Task Info":{"Task ID":94,"Index":0,"Attempt":0,"Launch Time":1444384125947,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":94,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":94,"Index":0,"Attempt":0,"Launch Time":1444384125947,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384126024,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":52,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599095,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384125947,"Completion Time":1444384126024,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":94,"Completion Time":1444384126024,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":95,"Submission Time":1444384126072,"Stage Infos":[{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[95],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":95,"Stage Attempt ID":0,"Task Info":{"Task ID":95,"Index":0,"Attempt":0,"Launch Time":1444384126080,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":95,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":95,"Index":0,"Attempt":0,"Launch Time":1444384126080,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384126147,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":47,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599095,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384126079,"Completion Time":1444384126147,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":95,"Completion Time":1444384126147,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":96,"Submission Time":1444384126194,"Stage Infos":[{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[96],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"628\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"628\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":96,"Stage Attempt ID":0,"Task Info":{"Task ID":96,"Index":0,"Attempt":0,"Launch Time":1444384126244,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":96,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":96,"Index":0,"Attempt":0,"Launch Time":1444384126244,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384126479,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":102,"Executor Run Time":125,"Result Size":1573,"JVM GC Time":21,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586779,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384126244,"Completion Time":1444384126479,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":96,"Completion Time":1444384126479,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":97,"Submission Time":1444384126530,"Stage Infos":[{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[97],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":97,"Stage Attempt ID":0,"Task Info":{"Task ID":97,"Index":0,"Attempt":0,"Launch Time":1444384126537,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":97,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":97,"Index":0,"Attempt":0,"Launch Time":1444384126537,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384126601,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":42,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599075,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384126536,"Completion Time":1444384126601,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":97,"Completion Time":1444384126601,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":98,"Submission Time":1444384126650,"Stage Infos":[{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[98],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":98,"Stage Attempt ID":0,"Task Info":{"Task ID":98,"Index":0,"Attempt":0,"Launch Time":1444384126656,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":98,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":98,"Index":0,"Attempt":0,"Launch Time":1444384126656,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384126723,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":10,"Executor Run Time":48,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599075,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384126656,"Completion Time":1444384126724,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":98,"Completion Time":1444384126724,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":99,"Submission Time":1444384126772,"Stage Infos":[{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[99],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":99,"Stage Attempt ID":0,"Task Info":{"Task ID":99,"Index":0,"Attempt":0,"Launch Time":1444384126777,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":99,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":99,"Index":0,"Attempt":0,"Launch Time":1444384126777,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444384126846,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":45,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599075,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444384126777,"Completion Time":1444384126846,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":99,"Completion Time":1444384126846,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1444384126868}
