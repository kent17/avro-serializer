{"Event":"SparkListenerLogStart","Spark Version":"1.4.1"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"192.168.2.68","Port":52442},"Maximum Memory":278019440,"Timestamp":1444397461334}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre","Java Version":"1.8.0_51 (Oracle Corporation)","Scala Version":"version 2.10.4"},"Spark Properties":{"spark.serializer":"org.apache.spark.serializer.KryoSerializer","spark.executor.extraJavaOptions":"-XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"","spark.driver.host":"192.168.2.68","spark.eventLog.enabled":"true","spark.driver.port":"52439","spark.jars":"file:/Users/quentin/Dev/avro-serializer/target/scala-2.10/sparkavroserializer_2.10-1.0.jar","spark.app.name":"SparkAvroSerializer","spark.scheduler.mode":"FIFO","spark.executor.id":"driver","spark.master":"spark://Quentins-MacBook-Pro.local:7077","spark.eventLog.dir":"spark-events","spark.fileserver.uri":"http://192.168.2.68:52440","spark.externalBlockStore.folderName":"spark-d9d8f2ab-b68b-4a57-bf51-9509bfaadd38","spark.app.id":"app-20151009153101-0004"},"System Properties":{"java.io.tmpdir":"/var/folders/hh/nlyyqy1j20x3xq4y7dflp6000000gn/T/","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/Users/quentin","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","ftp.nonProxyHosts":"local|*.local|169.254/16|*.169.254/16","sun.arch.data.model":"64","sun.boot.library.path":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib","user.dir":"/Users/quentin/Dev/avro-serializer","java.library.path":"/Users/quentin/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.","sun.cpu.isalist":"","os.arch":"x86_64","java.vm.version":"25.51-b03","java.endorsed.dirs":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/endorsed","java.runtime.version":"1.8.0_51-b16","java.vm.info":"mixed mode","java.ext.dirs":"/Users/quentin/Library/Java/Extensions:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/classes","file.encoding":"UTF-8","user.timezone":"Europe/Paris","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"10.10.5","sun.os.patch.level":"unknown","gopherProxySet":"false","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","http.nonProxyHosts":"local|*.local|169.254/16|*.169.254/16","user.language":"en","socksNonProxyHosts":"local|*.local|169.254/16|*.169.254/16","user.country.format":"FR","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.lwawt.macosx.CPrinterJob","java.awt.graphicsenv":"sun.awt.CGraphicsEnvironment","awt.toolkit":"sun.lwawt.macosx.LWCToolkit","os.name":"Mac OS X","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"quentin","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit --master spark://Quentins-MacBook-Pro.local:7077 --class com.avroserializer.SparkAvroExample target/scala-2.10/sparkavroserializer_2.10-1.0.jar","java.home":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre","java.version":"1.8.0_51","sun.io.unicode.encoding":"UnicodeBig"},"Classpath Entries":{"/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/spark-assembly-1.4.1-hadoop2.6.0.jar":"System Classpath","http://192.168.2.68:52440/jars/sparkavroserializer_2.10-1.0.jar":"Added By User","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/conf/":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"SparkAvroSerializer","App ID":"app-20151009153101-0004","Timestamp":1444397458940,"User":"quentin"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1444397463923,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"4\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"4\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1444397466855,"Executor ID":"0","Executor Info":{"Host":"192.168.2.68","Total Cores":1,"Log Urls":{"stdout":"http://192.168.2.68:8082/logPage/?appId=app-20151009153101-0004&executorId=0&logType=stdout","stderr":"http://192.168.2.68:8082/logPage/?appId=app-20151009153101-0004&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1444397466861,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1444397467035,"Executor ID":"1","Executor Info":{"Host":"192.168.2.68","Total Cores":1,"Log Urls":{"stdout":"http://192.168.2.68:8081/logPage/?appId=app-20151009153101-0004&executorId=1&logType=stdout","stderr":"http://192.168.2.68:8081/logPage/?appId=app-20151009153101-0004&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.2.68","Port":52453},"Maximum Memory":278019440,"Timestamp":1444397467151}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.2.68","Port":52454},"Maximum Memory":278019440,"Timestamp":1444397467282}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1444397466861,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397469465,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":1224,"Executor Run Time":977,"Result Size":1573,"JVM GC Time":96,"Result Serialization Time":2,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586780,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397464717,"Completion Time":1444397469471,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1444397469482,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1444397469624,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[1],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1444397469648,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1444397469648,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397471005,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":695,"Executor Run Time":602,"Result Size":1754,"JVM GC Time":63,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397469647,"Completion Time":1444397471006,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1444397471006,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1444397471102,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[2],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1444397471118,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1444397471118,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397471474,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":52,"Executor Run Time":280,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599076,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397471118,"Completion Time":1444397471474,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1444397471474,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":3,"Submission Time":1444397471561,"Stage Infos":[{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[3],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":0,"Attempt":0,"Launch Time":1444397471574,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":3,"Index":0,"Attempt":0,"Launch Time":1444397471574,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397471731,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":43,"Executor Run Time":94,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599076,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397471574,"Completion Time":1444397471733,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":3,"Completion Time":1444397471733,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":4,"Submission Time":1444397471877,"Stage Infos":[{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[4],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"30\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"30\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":4,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1444397471908,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":4,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1444397471908,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397472359,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":123,"Executor Run Time":304,"Result Size":1573,"JVM GC Time":7,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586800,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397471908,"Completion Time":1444397472359,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":4,"Completion Time":1444397472359,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":5,"Submission Time":1444397472432,"Stage Infos":[{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[5],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":5,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":0,"Attempt":0,"Launch Time":1444397472441,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":5,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":5,"Index":0,"Attempt":0,"Launch Time":1444397472441,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397472574,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":27,"Executor Run Time":78,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397472440,"Completion Time":1444397472575,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":5,"Completion Time":1444397472575,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":6,"Submission Time":1444397472654,"Stage Infos":[{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[6],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":6,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Launch Time":1444397472672,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":6,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Launch Time":1444397472672,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397472790,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":31,"Executor Run Time":74,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397472672,"Completion Time":1444397472791,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":6,"Completion Time":1444397472791,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":7,"Submission Time":1444397472853,"Stage Infos":[{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[7],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":7,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":0,"Attempt":0,"Launch Time":1444397472873,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":7,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":7,"Index":0,"Attempt":0,"Launch Time":1444397472873,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397472991,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":31,"Executor Run Time":75,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397472872,"Completion Time":1444397472992,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":7,"Completion Time":1444397472992,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":8,"Submission Time":1444397473065,"Stage Infos":[{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[8],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"56\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"56\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":8,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1444397473087,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":8,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1444397473087,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397473238,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":67,"Executor Run Time":62,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586756,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397473086,"Completion Time":1444397473239,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":8,"Completion Time":1444397473239,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":9,"Submission Time":1444397473294,"Stage Infos":[{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[9],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":9,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Launch Time":1444397473317,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":9,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Launch Time":1444397473317,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397473425,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":73,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599052,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397473316,"Completion Time":1444397473425,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":9,"Completion Time":1444397473425,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":10,"Submission Time":1444397473534,"Stage Infos":[{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[10],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":10,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Launch Time":1444397473555,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":10,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Launch Time":1444397473555,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397473724,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":44,"Executor Run Time":110,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599052,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397473555,"Completion Time":1444397473724,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":10,"Completion Time":1444397473724,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":11,"Submission Time":1444397473788,"Stage Infos":[{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[11],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":11,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Launch Time":1444397473796,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":11,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Launch Time":1444397473796,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397473901,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":69,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599052,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397473796,"Completion Time":1444397473902,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":11,"Completion Time":1444397473902,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":12,"Submission Time":1444397473972,"Stage Infos":[{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[12],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"82\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397473994,"Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"82\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":12,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1444397473994,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":12,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1444397473994,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397474252,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":88,"Executor Run Time":147,"Result Size":1573,"JVM GC Time":6,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586804,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397473994,"Completion Time":1444397474254,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":12,"Completion Time":1444397474254,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":13,"Submission Time":1444397474319,"Stage Infos":[{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[13],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":13,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":0,"Attempt":0,"Launch Time":1444397474336,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":13,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":13,"Index":0,"Attempt":0,"Launch Time":1444397474336,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397474430,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":64,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599100,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397474336,"Completion Time":1444397474430,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":13,"Completion Time":1444397474431,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":14,"Submission Time":1444397474488,"Stage Infos":[{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[14],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":14,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":0,"Attempt":0,"Launch Time":1444397474497,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":14,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":14,"Index":0,"Attempt":0,"Launch Time":1444397474497,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397474637,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":32,"Executor Run Time":77,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599100,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397474496,"Completion Time":1444397474638,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":14,"Completion Time":1444397474638,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":15,"Submission Time":1444397474702,"Stage Infos":[{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[15],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":15,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":0,"Attempt":0,"Launch Time":1444397474716,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":15,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":15,"Index":0,"Attempt":0,"Launch Time":1444397474716,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397474816,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":65,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599100,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397474716,"Completion Time":1444397474817,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":15,"Completion Time":1444397474817,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":16,"Submission Time":1444397474872,"Stage Infos":[{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[16],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"108\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"108\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":16,"Stage Attempt ID":0,"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1444397474893,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":16,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1444397474893,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397475071,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":51,"Executor Run Time":113,"Result Size":1573,"JVM GC Time":6,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586764,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397474893,"Completion Time":1444397475072,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":16,"Completion Time":1444397475072,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":17,"Submission Time":1444397475136,"Stage Infos":[{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[17],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":17,"Stage Attempt ID":0,"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1444397475156,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":17,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1444397475156,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397475252,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":25,"Executor Run Time":58,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397475156,"Completion Time":1444397475253,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":17,"Completion Time":1444397475253,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":18,"Submission Time":1444397475308,"Stage Infos":[{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[18],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":18,"Stage Attempt ID":0,"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1444397475320,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":18,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1444397475320,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397475426,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":20,"Executor Run Time":71,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397475319,"Completion Time":1444397475427,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":18,"Completion Time":1444397475427,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":19,"Submission Time":1444397475484,"Stage Infos":[{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[19],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":19,"Index":0,"Attempt":0,"Launch Time":1444397475495,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":19,"Index":0,"Attempt":0,"Launch Time":1444397475495,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397475590,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":21,"Executor Run Time":62,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397475494,"Completion Time":1444397475590,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":19,"Completion Time":1444397475590,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":20,"Submission Time":1444397475875,"Stage Infos":[{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[20],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"134\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"134\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":20,"Stage Attempt ID":0,"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1444397475900,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":20,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1444397475900,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397476263,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":54,"Executor Run Time":296,"Result Size":1573,"JVM GC Time":32,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586782,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397475900,"Completion Time":1444397476263,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":20,"Completion Time":1444397476263,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":21,"Submission Time":1444397476321,"Stage Infos":[{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[21],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":21,"Stage Attempt ID":0,"Task Info":{"Task ID":21,"Index":0,"Attempt":0,"Launch Time":1444397476337,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":21,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":21,"Index":0,"Attempt":0,"Launch Time":1444397476337,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397476454,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":81,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397476336,"Completion Time":1444397476455,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":21,"Completion Time":1444397476455,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":22,"Submission Time":1444397476507,"Stage Infos":[{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[22],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":22,"Stage Attempt ID":0,"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1444397476517,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":22,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1444397476517,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397476616,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":70,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397476516,"Completion Time":1444397476617,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":22,"Completion Time":1444397476617,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":23,"Submission Time":1444397476666,"Stage Infos":[{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[23],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":23,"Stage Attempt ID":0,"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Launch Time":1444397476675,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":23,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Launch Time":1444397476675,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397476796,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":88,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397476675,"Completion Time":1444397476796,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":23,"Completion Time":1444397476796,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":24,"Submission Time":1444397476855,"Stage Infos":[{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[24],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"160\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"160\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1444397476875,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1444397476875,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397477091,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":65,"Executor Run Time":138,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586777,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397476875,"Completion Time":1444397477092,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":24,"Completion Time":1444397477092,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":25,"Submission Time":1444397477157,"Stage Infos":[{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[25],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":25,"Stage Attempt ID":0,"Task Info":{"Task ID":25,"Index":0,"Attempt":0,"Launch Time":1444397477175,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":25,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":25,"Index":0,"Attempt":0,"Launch Time":1444397477175,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397477278,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":65,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599073,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397477175,"Completion Time":1444397477283,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":25,"Completion Time":1444397477283,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":26,"Submission Time":1444397477335,"Stage Infos":[{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[26],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":26,"Stage Attempt ID":0,"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1444397477354,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":26,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1444397477354,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397477455,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":72,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599073,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397477353,"Completion Time":1444397477456,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":26,"Completion Time":1444397477456,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":27,"Submission Time":1444397477504,"Stage Infos":[{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[27],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":27,"Stage Attempt ID":0,"Task Info":{"Task ID":27,"Index":0,"Attempt":0,"Launch Time":1444397477516,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":27,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":27,"Index":0,"Attempt":0,"Launch Time":1444397477516,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397477617,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":66,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599073,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397477516,"Completion Time":1444397477618,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":27,"Completion Time":1444397477618,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":28,"Submission Time":1444397477668,"Stage Infos":[{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[28],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"186\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"186\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":28,"Stage Attempt ID":0,"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1444397477684,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":28,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1444397477684,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397477868,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":49,"Executor Run Time":116,"Result Size":1573,"JVM GC Time":15,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586779,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397477684,"Completion Time":1444397477869,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":28,"Completion Time":1444397477869,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":29,"Submission Time":1444397477925,"Stage Infos":[{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[29],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":29,"Stage Attempt ID":0,"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Launch Time":1444397477948,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":29,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Launch Time":1444397477948,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397478120,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":138,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599075,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397477948,"Completion Time":1444397478121,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":29,"Completion Time":1444397478121,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":30,"Submission Time":1444397478174,"Stage Infos":[{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[30],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":30,"Stage Attempt ID":0,"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1444397478194,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":30,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1444397478194,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397478276,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":20,"Executor Run Time":52,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599075,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397478194,"Completion Time":1444397478277,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":30,"Completion Time":1444397478277,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":31,"Submission Time":1444397478328,"Stage Infos":[{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[31],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":31,"Stage Attempt ID":0,"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1444397478336,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":31,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1444397478336,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397478424,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":20,"Executor Run Time":55,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599075,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397478336,"Completion Time":1444397478425,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":31,"Completion Time":1444397478426,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":32,"Submission Time":1444397478476,"Stage Infos":[{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[32],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"212\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"212\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":32,"Stage Attempt ID":0,"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1444397478495,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":32,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1444397478495,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397478675,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":47,"Executor Run Time":122,"Result Size":1573,"JVM GC Time":12,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586780,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397478495,"Completion Time":1444397478676,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":32,"Completion Time":1444397478676,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":33,"Submission Time":1444397478728,"Stage Infos":[{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[33],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":33,"Stage Attempt ID":0,"Task Info":{"Task ID":33,"Index":0,"Attempt":0,"Launch Time":1444397478736,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":33,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":33,"Index":0,"Attempt":0,"Launch Time":1444397478736,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397478818,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":53,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599076,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397478735,"Completion Time":1444397478819,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":33,"Completion Time":1444397478819,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":34,"Submission Time":1444397478862,"Stage Infos":[{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[34],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":34,"Stage Attempt ID":0,"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1444397478875,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":34,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1444397478875,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397478967,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":67,"Result Size":1753,"JVM GC Time":13,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599076,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397478875,"Completion Time":1444397478968,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":34,"Completion Time":1444397478968,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":35,"Submission Time":1444397479016,"Stage Infos":[{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[35],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":35,"Stage Attempt ID":0,"Task Info":{"Task ID":35,"Index":0,"Attempt":0,"Launch Time":1444397479033,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":35,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":35,"Index":0,"Attempt":0,"Launch Time":1444397479033,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397479113,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":52,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599076,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397479033,"Completion Time":1444397479114,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":35,"Completion Time":1444397479114,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":36,"Submission Time":1444397479164,"Stage Infos":[{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[36],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"238\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"238\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":36,"Stage Attempt ID":0,"Task Info":{"Task ID":36,"Index":0,"Attempt":0,"Launch Time":1444397479204,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":36,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":36,"Index":0,"Attempt":0,"Launch Time":1444397479204,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397479475,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":119,"Executor Run Time":140,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586782,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397479204,"Completion Time":1444397479475,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":36,"Completion Time":1444397479475,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":37,"Submission Time":1444397479525,"Stage Infos":[{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[37],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":37,"Stage Attempt ID":0,"Task Info":{"Task ID":37,"Index":0,"Attempt":0,"Launch Time":1444397479535,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":37,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":37,"Index":0,"Attempt":0,"Launch Time":1444397479535,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397479612,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":49,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397479535,"Completion Time":1444397479613,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":37,"Completion Time":1444397479613,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":38,"Submission Time":1444397479673,"Stage Infos":[{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[38],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":38,"Stage Attempt ID":0,"Task Info":{"Task ID":38,"Index":0,"Attempt":0,"Launch Time":1444397479695,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":38,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":38,"Index":0,"Attempt":0,"Launch Time":1444397479695,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397479776,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":52,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397479694,"Completion Time":1444397479777,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":38,"Completion Time":1444397479777,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":39,"Submission Time":1444397479825,"Stage Infos":[{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[39],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":39,"Stage Attempt ID":0,"Task Info":{"Task ID":39,"Index":0,"Attempt":0,"Launch Time":1444397479838,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":39,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":39,"Index":0,"Attempt":0,"Launch Time":1444397479838,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397479910,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":47,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397479837,"Completion Time":1444397479910,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":39,"Completion Time":1444397479910,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":40,"Submission Time":1444397479965,"Stage Infos":[{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[40],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"264\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"264\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":40,"Stage Attempt ID":0,"Task Info":{"Task ID":40,"Index":0,"Attempt":0,"Launch Time":1444397479980,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":40,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":40,"Index":0,"Attempt":0,"Launch Time":1444397479980,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397480145,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":46,"Executor Run Time":99,"Result Size":1573,"JVM GC Time":8,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586783,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397479979,"Completion Time":1444397480146,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":40,"Completion Time":1444397480146,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":41,"Submission Time":1444397480205,"Stage Infos":[{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[41],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":41,"Stage Attempt ID":0,"Task Info":{"Task ID":41,"Index":0,"Attempt":0,"Launch Time":1444397480217,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":41,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":41,"Index":0,"Attempt":0,"Launch Time":1444397480217,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397480292,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":48,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599079,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397480217,"Completion Time":1444397480293,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":41,"Completion Time":1444397480293,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":42,"Submission Time":1444397480341,"Stage Infos":[{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[42],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":42,"Stage Attempt ID":0,"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Launch Time":1444397480356,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":42,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Launch Time":1444397480356,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397480436,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":53,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599079,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397480355,"Completion Time":1444397480436,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":42,"Completion Time":1444397480436,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":43,"Submission Time":1444397480485,"Stage Infos":[{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[43],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":43,"Stage Attempt ID":0,"Task Info":{"Task ID":43,"Index":0,"Attempt":0,"Launch Time":1444397480512,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":43,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":43,"Index":0,"Attempt":0,"Launch Time":1444397480512,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397480658,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":31,"Executor Run Time":105,"Result Size":1754,"JVM GC Time":20,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599079,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397480512,"Completion Time":1444397480658,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":43,"Completion Time":1444397480658,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":44,"Submission Time":1444397480710,"Stage Infos":[{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[44],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"290\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"290\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":44,"Stage Attempt ID":0,"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Launch Time":1444397480728,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":44,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Launch Time":1444397480728,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397480930,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":46,"Executor Run Time":140,"Result Size":1573,"JVM GC Time":18,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586761,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397480727,"Completion Time":1444397480930,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":44,"Completion Time":1444397480931,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":45,"Submission Time":1444397480991,"Stage Infos":[{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[45],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":45,"Stage Attempt ID":0,"Task Info":{"Task ID":45,"Index":0,"Attempt":0,"Launch Time":1444397480997,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":45,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":45,"Index":0,"Attempt":0,"Launch Time":1444397480997,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397481083,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":59,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599057,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397480997,"Completion Time":1444397481084,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":45,"Completion Time":1444397481084,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":46,"Submission Time":1444397481131,"Stage Infos":[{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[46],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":46,"Stage Attempt ID":0,"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Launch Time":1444397481139,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":46,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Launch Time":1444397481139,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397481210,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":47,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599057,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397481139,"Completion Time":1444397481210,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":46,"Completion Time":1444397481210,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":47,"Submission Time":1444397481261,"Stage Infos":[{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[47],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":47,"Stage Attempt ID":0,"Task Info":{"Task ID":47,"Index":0,"Attempt":0,"Launch Time":1444397481274,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":47,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":47,"Index":0,"Attempt":0,"Launch Time":1444397481274,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397481350,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":52,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599057,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397481274,"Completion Time":1444397481351,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":47,"Completion Time":1444397481351,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":48,"Submission Time":1444397481402,"Stage Infos":[{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[48],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"316\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"316\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":48,"Stage Attempt ID":0,"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Launch Time":1444397481417,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":48,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Launch Time":1444397481417,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397481616,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":42,"Executor Run Time":145,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586773,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397481417,"Completion Time":1444397481618,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":48,"Completion Time":1444397481618,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":49,"Submission Time":1444397481672,"Stage Infos":[{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[49],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":49,"Stage Attempt ID":0,"Task Info":{"Task ID":49,"Index":0,"Attempt":0,"Launch Time":1444397481695,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":49,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":49,"Index":0,"Attempt":0,"Launch Time":1444397481695,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397481768,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":50,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397481695,"Completion Time":1444397481768,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":49,"Completion Time":1444397481768,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":50,"Submission Time":1444397481814,"Stage Infos":[{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[50],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":50,"Stage Attempt ID":0,"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Launch Time":1444397481833,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":50,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Launch Time":1444397481833,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397482019,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":41,"Executor Run Time":112,"Result Size":1753,"JVM GC Time":18,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397481833,"Completion Time":1444397482020,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":50,"Completion Time":1444397482020,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":51,"Submission Time":1444397482064,"Stage Infos":[{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[51],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":51,"Stage Attempt ID":0,"Task Info":{"Task ID":51,"Index":0,"Attempt":0,"Launch Time":1444397482075,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":51,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":51,"Index":0,"Attempt":0,"Launch Time":1444397482075,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397482146,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":46,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397482075,"Completion Time":1444397482146,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":51,"Completion Time":1444397482146,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":52,"Submission Time":1444397482201,"Stage Infos":[{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[52],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"342\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"342\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":52,"Stage Attempt ID":0,"Task Info":{"Task ID":52,"Index":0,"Attempt":0,"Launch Time":1444397482221,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":52,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":52,"Index":0,"Attempt":0,"Launch Time":1444397482221,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397482433,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":50,"Executor Run Time":151,"Result Size":1573,"JVM GC Time":16,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586790,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397482221,"Completion Time":1444397482434,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":52,"Completion Time":1444397482434,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":53,"Submission Time":1444397482495,"Stage Infos":[{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[53],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":53,"Stage Attempt ID":0,"Task Info":{"Task ID":53,"Index":0,"Attempt":0,"Launch Time":1444397482515,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":53,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":53,"Index":0,"Attempt":0,"Launch Time":1444397482515,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397482597,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":21,"Executor Run Time":51,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397482515,"Completion Time":1444397482598,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":53,"Completion Time":1444397482598,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":54,"Submission Time":1444397482646,"Stage Infos":[{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[54],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":54,"Stage Attempt ID":0,"Task Info":{"Task ID":54,"Index":0,"Attempt":0,"Launch Time":1444397482656,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":54,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":54,"Index":0,"Attempt":0,"Launch Time":1444397482656,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397482729,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":47,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397482655,"Completion Time":1444397482730,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":54,"Completion Time":1444397482730,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":55,"Submission Time":1444397482773,"Stage Infos":[{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[55],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":55,"Stage Attempt ID":0,"Task Info":{"Task ID":55,"Index":0,"Attempt":0,"Launch Time":1444397482794,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":55,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":55,"Index":0,"Attempt":0,"Launch Time":1444397482794,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397482857,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":42,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397482793,"Completion Time":1444397482857,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":55,"Completion Time":1444397482857,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":56,"Submission Time":1444397482905,"Stage Infos":[{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[56],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"368\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"368\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":56,"Stage Attempt ID":0,"Task Info":{"Task ID":56,"Index":0,"Attempt":0,"Launch Time":1444397482923,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":56,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":56,"Index":0,"Attempt":0,"Launch Time":1444397482923,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397483075,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":41,"Executor Run Time":94,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586770,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397482922,"Completion Time":1444397483075,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":56,"Completion Time":1444397483075,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":57,"Submission Time":1444397483128,"Stage Infos":[{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[57],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":57,"Stage Attempt ID":0,"Task Info":{"Task ID":57,"Index":0,"Attempt":0,"Launch Time":1444397483135,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":57,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":57,"Index":0,"Attempt":0,"Launch Time":1444397483135,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397483200,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":43,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397483135,"Completion Time":1444397483201,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":57,"Completion Time":1444397483201,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":58,"Submission Time":1444397483301,"Stage Infos":[{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[58],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":58,"Stage Attempt ID":0,"Task Info":{"Task ID":58,"Index":0,"Attempt":0,"Launch Time":1444397483335,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":58,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":58,"Index":0,"Attempt":0,"Launch Time":1444397483335,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397483439,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":33,"Executor Run Time":61,"Result Size":1753,"JVM GC Time":15,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397483335,"Completion Time":1444397483440,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":58,"Completion Time":1444397483440,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":59,"Submission Time":1444397483492,"Stage Infos":[{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[59],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":59,"Stage Attempt ID":0,"Task Info":{"Task ID":59,"Index":0,"Attempt":0,"Launch Time":1444397483498,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":59,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":59,"Index":0,"Attempt":0,"Launch Time":1444397483498,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397483582,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":45,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397483497,"Completion Time":1444397483582,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":59,"Completion Time":1444397483582,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":60,"Submission Time":1444397483633,"Stage Infos":[{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[60],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"394\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"394\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":60,"Stage Attempt ID":0,"Task Info":{"Task ID":60,"Index":0,"Attempt":0,"Launch Time":1444397483653,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":60,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":60,"Index":0,"Attempt":0,"Launch Time":1444397483653,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397483818,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":42,"Executor Run Time":95,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586786,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397483653,"Completion Time":1444397483819,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":60,"Completion Time":1444397483819,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":61,"Submission Time":1444397483873,"Stage Infos":[{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[61],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":61,"Stage Attempt ID":0,"Task Info":{"Task ID":61,"Index":0,"Attempt":0,"Launch Time":1444397483893,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":61,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":61,"Index":0,"Attempt":0,"Launch Time":1444397483893,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397483960,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":44,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599082,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397483893,"Completion Time":1444397483961,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":61,"Completion Time":1444397483961,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":62,"Submission Time":1444397484006,"Stage Infos":[{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[62],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":62,"Stage Attempt ID":0,"Task Info":{"Task ID":62,"Index":0,"Attempt":0,"Launch Time":1444397484014,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":62,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":62,"Index":0,"Attempt":0,"Launch Time":1444397484014,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397484080,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":43,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599082,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397484013,"Completion Time":1444397484081,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":62,"Completion Time":1444397484081,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":63,"Submission Time":1444397484130,"Stage Infos":[{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[63],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":63,"Stage Attempt ID":0,"Task Info":{"Task ID":63,"Index":0,"Attempt":0,"Launch Time":1444397484138,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":63,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":63,"Index":0,"Attempt":0,"Launch Time":1444397484138,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397484200,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":41,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599082,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397484138,"Completion Time":1444397484201,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":63,"Completion Time":1444397484201,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":64,"Submission Time":1444397484243,"Stage Infos":[{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[64],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"420\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"420\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":64,"Stage Attempt ID":0,"Task Info":{"Task ID":64,"Index":0,"Attempt":0,"Launch Time":1444397484260,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":64,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":64,"Index":0,"Attempt":0,"Launch Time":1444397484260,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397484407,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":37,"Executor Run Time":90,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586789,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397484259,"Completion Time":1444397484408,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":64,"Completion Time":1444397484408,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":65,"Submission Time":1444397484460,"Stage Infos":[{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[65],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":65,"Stage Attempt ID":0,"Task Info":{"Task ID":65,"Index":0,"Attempt":0,"Launch Time":1444397484490,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":65,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":65,"Index":0,"Attempt":0,"Launch Time":1444397484490,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397484629,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":47,"Executor Run Time":85,"Result Size":1754,"JVM GC Time":8,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599085,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397484490,"Completion Time":1444397484630,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":65,"Completion Time":1444397484630,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":66,"Submission Time":1444397484675,"Stage Infos":[{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[66],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":66,"Stage Attempt ID":0,"Task Info":{"Task ID":66,"Index":0,"Attempt":0,"Launch Time":1444397484694,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":66,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":66,"Index":0,"Attempt":0,"Launch Time":1444397484694,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397484769,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":51,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599085,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397484694,"Completion Time":1444397484771,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":66,"Completion Time":1444397484771,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":67,"Submission Time":1444397484813,"Stage Infos":[{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[67],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":67,"Stage Attempt ID":0,"Task Info":{"Task ID":67,"Index":0,"Attempt":0,"Launch Time":1444397484833,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":67,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":67,"Index":0,"Attempt":0,"Launch Time":1444397484833,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397484900,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":45,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599085,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397484833,"Completion Time":1444397484901,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":67,"Completion Time":1444397484901,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":68,"Submission Time":1444397484948,"Stage Infos":[{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[68],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"446\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"446\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":68,"Stage Attempt ID":0,"Task Info":{"Task ID":68,"Index":0,"Attempt":0,"Launch Time":1444397484962,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":68,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":68,"Index":0,"Attempt":0,"Launch Time":1444397484962,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397485113,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":33,"Executor Run Time":101,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586750,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397484962,"Completion Time":1444397485113,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":68,"Completion Time":1444397485113,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":69,"Submission Time":1444397485164,"Stage Infos":[{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[69],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":69,"Stage Attempt ID":0,"Task Info":{"Task ID":69,"Index":0,"Attempt":0,"Launch Time":1444397485175,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":69,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":69,"Index":0,"Attempt":0,"Launch Time":1444397485175,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397485241,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":41,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599046,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397485175,"Completion Time":1444397485241,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":69,"Completion Time":1444397485241,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":70,"Submission Time":1444397485290,"Stage Infos":[{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[70],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":70,"Stage Attempt ID":0,"Task Info":{"Task ID":70,"Index":0,"Attempt":0,"Launch Time":1444397485296,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":70,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":70,"Index":0,"Attempt":0,"Launch Time":1444397485296,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397485362,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":47,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599046,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397485295,"Completion Time":1444397485362,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":70,"Completion Time":1444397485362,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":71,"Submission Time":1444397485410,"Stage Infos":[{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[71],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":71,"Stage Attempt ID":0,"Task Info":{"Task ID":71,"Index":0,"Attempt":0,"Launch Time":1444397485416,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":71,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":71,"Index":0,"Attempt":0,"Launch Time":1444397485416,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397485482,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":42,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599046,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397485415,"Completion Time":1444397485482,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":71,"Completion Time":1444397485482,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":72,"Submission Time":1444397485537,"Stage Infos":[{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[72],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"472\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"472\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":72,"Stage Attempt ID":0,"Task Info":{"Task ID":72,"Index":0,"Attempt":0,"Launch Time":1444397485556,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":72,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":72,"Index":0,"Attempt":0,"Launch Time":1444397485556,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397485717,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":37,"Executor Run Time":116,"Result Size":1573,"JVM GC Time":19,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586765,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397485556,"Completion Time":1444397485718,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":72,"Completion Time":1444397485718,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":73,"Submission Time":1444397485814,"Stage Infos":[{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[73],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":73,"Stage Attempt ID":0,"Task Info":{"Task ID":73,"Index":0,"Attempt":0,"Launch Time":1444397485839,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":73,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":73,"Index":0,"Attempt":0,"Launch Time":1444397485839,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397485917,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":20,"Executor Run Time":36,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599061,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397485839,"Completion Time":1444397485917,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":73,"Completion Time":1444397485918,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":74,"Submission Time":1444397485963,"Stage Infos":[{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[74],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":74,"Stage Attempt ID":0,"Task Info":{"Task ID":74,"Index":0,"Attempt":0,"Launch Time":1444397485975,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":74,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":74,"Index":0,"Attempt":0,"Launch Time":1444397485975,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397486037,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":40,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599061,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397485975,"Completion Time":1444397486038,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":74,"Completion Time":1444397486038,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":75,"Submission Time":1444397486080,"Stage Infos":[{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[75],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":75,"Stage Attempt ID":0,"Task Info":{"Task ID":75,"Index":0,"Attempt":0,"Launch Time":1444397486094,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":75,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":75,"Index":0,"Attempt":0,"Launch Time":1444397486094,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397486158,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":41,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599061,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397486094,"Completion Time":1444397486158,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":75,"Completion Time":1444397486158,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":76,"Submission Time":1444397486209,"Stage Infos":[{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[76],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"498\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"498\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":76,"Stage Attempt ID":0,"Task Info":{"Task ID":76,"Index":0,"Attempt":0,"Launch Time":1444397486227,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":76,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":76,"Index":0,"Attempt":0,"Launch Time":1444397486227,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397486508,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":60,"Executor Run Time":202,"Result Size":1573,"JVM GC Time":18,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586797,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397486227,"Completion Time":1444397486508,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":76,"Completion Time":1444397486509,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":77,"Submission Time":1444397486571,"Stage Infos":[{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[77],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":77,"Stage Attempt ID":0,"Task Info":{"Task ID":77,"Index":0,"Attempt":0,"Launch Time":1444397486577,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":77,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":77,"Index":0,"Attempt":0,"Launch Time":1444397486577,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397486639,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":39,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599093,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397486576,"Completion Time":1444397486640,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":77,"Completion Time":1444397486640,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":78,"Submission Time":1444397486691,"Stage Infos":[{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[78],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":78,"Stage Attempt ID":0,"Task Info":{"Task ID":78,"Index":0,"Attempt":0,"Launch Time":1444397486698,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":78,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":78,"Index":0,"Attempt":0,"Launch Time":1444397486698,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397486777,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":42,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599093,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397486698,"Completion Time":1444397486778,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":78,"Completion Time":1444397486778,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":79,"Submission Time":1444397486835,"Stage Infos":[{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[79],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":79,"Stage Attempt ID":0,"Task Info":{"Task ID":79,"Index":0,"Attempt":0,"Launch Time":1444397486856,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":79,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":79,"Index":0,"Attempt":0,"Launch Time":1444397486856,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397486924,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":46,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599093,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397486856,"Completion Time":1444397486925,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":79,"Completion Time":1444397486925,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":80,"Submission Time":1444397486967,"Stage Infos":[{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[80],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"524\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"524\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":80,"Stage Attempt ID":0,"Task Info":{"Task ID":80,"Index":0,"Attempt":0,"Launch Time":1444397486982,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":80,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":80,"Index":0,"Attempt":0,"Launch Time":1444397486982,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397487165,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":37,"Executor Run Time":129,"Result Size":1573,"JVM GC Time":20,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586804,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397486981,"Completion Time":1444397487166,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":80,"Completion Time":1444397487166,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":81,"Submission Time":1444397487254,"Stage Infos":[{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[81],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":81,"Stage Attempt ID":0,"Task Info":{"Task ID":81,"Index":0,"Attempt":0,"Launch Time":1444397487275,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":81,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":81,"Index":0,"Attempt":0,"Launch Time":1444397487275,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397487366,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":33,"Executor Run Time":51,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599100,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397487275,"Completion Time":1444397487366,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":81,"Completion Time":1444397487367,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":82,"Submission Time":1444397487411,"Stage Infos":[{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[82],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":82,"Stage Attempt ID":0,"Task Info":{"Task ID":82,"Index":0,"Attempt":0,"Launch Time":1444397487417,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":82,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":82,"Index":0,"Attempt":0,"Launch Time":1444397487417,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397487478,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":41,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599100,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397487417,"Completion Time":1444397487479,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":82,"Completion Time":1444397487479,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":83,"Submission Time":1444397487525,"Stage Infos":[{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[83],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":83,"Stage Attempt ID":0,"Task Info":{"Task ID":83,"Index":0,"Attempt":0,"Launch Time":1444397487535,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":83,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":83,"Index":0,"Attempt":0,"Launch Time":1444397487535,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397487596,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":41,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599100,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397487534,"Completion Time":1444397487596,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":83,"Completion Time":1444397487596,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":84,"Submission Time":1444397487644,"Stage Infos":[{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[84],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"550\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"550\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":84,"Stage Attempt ID":0,"Task Info":{"Task ID":84,"Index":0,"Attempt":0,"Launch Time":1444397487661,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":84,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":84,"Index":0,"Attempt":0,"Launch Time":1444397487661,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397487823,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":35,"Executor Run Time":105,"Result Size":1573,"JVM GC Time":16,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586764,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397487660,"Completion Time":1444397487824,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":84,"Completion Time":1444397487824,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":85,"Submission Time":1444397487879,"Stage Infos":[{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[85],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":85,"Stage Attempt ID":0,"Task Info":{"Task ID":85,"Index":0,"Attempt":0,"Launch Time":1444397487894,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":85,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":85,"Index":0,"Attempt":0,"Launch Time":1444397487894,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397487949,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":36,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397487894,"Completion Time":1444397487950,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":85,"Completion Time":1444397487950,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":86,"Submission Time":1444397487998,"Stage Infos":[{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[86],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":86,"Stage Attempt ID":0,"Task Info":{"Task ID":86,"Index":0,"Attempt":0,"Launch Time":1444397488015,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":86,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":86,"Index":0,"Attempt":0,"Launch Time":1444397488015,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397488075,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":37,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397488015,"Completion Time":1444397488075,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":86,"Completion Time":1444397488075,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":87,"Submission Time":1444397488120,"Stage Infos":[{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[87],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":87,"Stage Attempt ID":0,"Task Info":{"Task ID":87,"Index":0,"Attempt":0,"Launch Time":1444397488134,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":87,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":87,"Index":0,"Attempt":0,"Launch Time":1444397488134,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397488193,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397488134,"Completion Time":1444397488194,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":87,"Completion Time":1444397488194,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":88,"Submission Time":1444397488236,"Stage Infos":[{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[88],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"576\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"576\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":88,"Stage Attempt ID":0,"Task Info":{"Task ID":88,"Index":0,"Attempt":0,"Launch Time":1444397488256,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":88,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":88,"Index":0,"Attempt":0,"Launch Time":1444397488256,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397488448,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":36,"Executor Run Time":148,"Result Size":1573,"JVM GC Time":23,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586768,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397488256,"Completion Time":1444397488449,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":88,"Completion Time":1444397488449,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":89,"Submission Time":1444397488534,"Stage Infos":[{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[89],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":89,"Stage Attempt ID":0,"Task Info":{"Task ID":89,"Index":0,"Attempt":0,"Launch Time":1444397488557,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":89,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":89,"Index":0,"Attempt":0,"Launch Time":1444397488557,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397488651,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":45,"Executor Run Time":39,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599064,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397488557,"Completion Time":1444397488651,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":89,"Completion Time":1444397488651,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":90,"Submission Time":1444397488698,"Stage Infos":[{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[90],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":90,"Stage Attempt ID":0,"Task Info":{"Task ID":90,"Index":0,"Attempt":0,"Launch Time":1444397488715,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":90,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":90,"Index":0,"Attempt":0,"Launch Time":1444397488715,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397488780,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":43,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599064,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397488715,"Completion Time":1444397488781,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":90,"Completion Time":1444397488781,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":91,"Submission Time":1444397488826,"Stage Infos":[{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[91],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":91,"Stage Attempt ID":0,"Task Info":{"Task ID":91,"Index":0,"Attempt":0,"Launch Time":1444397488835,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":91,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":91,"Index":0,"Attempt":0,"Launch Time":1444397488835,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397488892,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":37,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599064,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397488834,"Completion Time":1444397488893,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":91,"Completion Time":1444397488893,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":92,"Submission Time":1444397488940,"Stage Infos":[{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[92],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"602\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"602\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":92,"Stage Attempt ID":0,"Task Info":{"Task ID":92,"Index":0,"Attempt":0,"Launch Time":1444397488957,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":92,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":92,"Index":0,"Attempt":0,"Launch Time":1444397488957,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397489153,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":34,"Executor Run Time":153,"Result Size":1573,"JVM GC Time":10,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586761,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397488956,"Completion Time":1444397489154,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":92,"Completion Time":1444397489154,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":93,"Submission Time":1444397489203,"Stage Infos":[{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[93],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":93,"Stage Attempt ID":0,"Task Info":{"Task ID":93,"Index":0,"Attempt":0,"Launch Time":1444397489215,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":93,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":93,"Index":0,"Attempt":0,"Launch Time":1444397489215,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397489275,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599057,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397489215,"Completion Time":1444397489275,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":93,"Completion Time":1444397489275,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":94,"Submission Time":1444397489320,"Stage Infos":[{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[94],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":94,"Stage Attempt ID":0,"Task Info":{"Task ID":94,"Index":0,"Attempt":0,"Launch Time":1444397489334,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":94,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":94,"Index":0,"Attempt":0,"Launch Time":1444397489334,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397489393,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":37,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599057,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397489334,"Completion Time":1444397489394,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":94,"Completion Time":1444397489394,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":95,"Submission Time":1444397489443,"Stage Infos":[{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[95],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":95,"Stage Attempt ID":0,"Task Info":{"Task ID":95,"Index":0,"Attempt":0,"Launch Time":1444397489455,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":95,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":95,"Index":0,"Attempt":0,"Launch Time":1444397489455,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397489515,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599057,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397489455,"Completion Time":1444397489516,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":95,"Completion Time":1444397489516,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":96,"Submission Time":1444397489559,"Stage Infos":[{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[96],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"628\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"628\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":96,"Stage Attempt ID":0,"Task Info":{"Task ID":96,"Index":0,"Attempt":0,"Launch Time":1444397489575,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":96,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":96,"Index":0,"Attempt":0,"Launch Time":1444397489575,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397489730,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":40,"Executor Run Time":107,"Result Size":1573,"JVM GC Time":14,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586790,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397489575,"Completion Time":1444397489731,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":96,"Completion Time":1444397489731,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":97,"Submission Time":1444397489817,"Stage Infos":[{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[97],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":97,"Stage Attempt ID":0,"Task Info":{"Task ID":97,"Index":0,"Attempt":0,"Launch Time":1444397489859,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":97,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":97,"Index":0,"Attempt":0,"Launch Time":1444397489859,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397489948,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":37,"Executor Run Time":44,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397489858,"Completion Time":1444397489948,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":97,"Completion Time":1444397489948,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":98,"Submission Time":1444397489994,"Stage Infos":[{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[98],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":98,"Stage Attempt ID":0,"Task Info":{"Task ID":98,"Index":0,"Attempt":0,"Launch Time":1444397490014,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":98,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":98,"Index":0,"Attempt":0,"Launch Time":1444397490014,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397490079,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":44,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397490014,"Completion Time":1444397490080,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":98,"Completion Time":1444397490080,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":99,"Submission Time":1444397490140,"Stage Infos":[{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[99],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":99,"Stage Attempt ID":0,"Task Info":{"Task ID":99,"Index":0,"Attempt":0,"Launch Time":1444397490155,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":99,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":99,"Index":0,"Attempt":0,"Launch Time":1444397490155,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397490213,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":10,"Executor Run Time":40,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599086,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397490155,"Completion Time":1444397490213,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":99,"Completion Time":1444397490213,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1444397490258}
