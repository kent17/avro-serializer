{"Event":"SparkListenerLogStart","Spark Version":"1.4.1"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"192.168.2.68","Port":52413},"Maximum Memory":278019440,"Timestamp":1444397315352}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre","Java Version":"1.8.0_51 (Oracle Corporation)","Scala Version":"version 2.10.4"},"Spark Properties":{"spark.serializer":"org.apache.spark.serializer.KryoSerializer","spark.executor.extraJavaOptions":"-XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"","spark.driver.host":"192.168.2.68","spark.eventLog.enabled":"true","spark.driver.port":"52410","spark.jars":"file:/Users/quentin/Dev/avro-serializer/target/scala-2.10/sparkavroserializer_2.10-1.0.jar","spark.app.name":"SparkAvroSerializer","spark.scheduler.mode":"FIFO","spark.executor.id":"driver","spark.master":"spark://Quentins-MacBook-Pro.local:7077","spark.eventLog.dir":"spark-events","spark.fileserver.uri":"http://192.168.2.68:52411","spark.externalBlockStore.folderName":"spark-fee45c2c-98fe-46ed-9531-89751367d783","spark.app.id":"app-20151009152834-0003"},"System Properties":{"java.io.tmpdir":"/var/folders/hh/nlyyqy1j20x3xq4y7dflp6000000gn/T/","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/Users/quentin","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","ftp.nonProxyHosts":"local|*.local|169.254/16|*.169.254/16","sun.arch.data.model":"64","sun.boot.library.path":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib","user.dir":"/Users/quentin/Dev/avro-serializer","java.library.path":"/Users/quentin/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.","sun.cpu.isalist":"","os.arch":"x86_64","java.vm.version":"25.51-b03","java.endorsed.dirs":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/endorsed","java.runtime.version":"1.8.0_51-b16","java.vm.info":"mixed mode","java.ext.dirs":"/Users/quentin/Library/Java/Extensions:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre/classes","file.encoding":"UTF-8","user.timezone":"Europe/Paris","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"10.10.5","sun.os.patch.level":"unknown","gopherProxySet":"false","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","http.nonProxyHosts":"local|*.local|169.254/16|*.169.254/16","user.language":"en","socksNonProxyHosts":"local|*.local|169.254/16|*.169.254/16","user.country.format":"FR","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.lwawt.macosx.CPrinterJob","java.awt.graphicsenv":"sun.awt.CGraphicsEnvironment","awt.toolkit":"sun.lwawt.macosx.LWCToolkit","os.name":"Mac OS X","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"quentin","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit --master spark://Quentins-MacBook-Pro.local:7077 --class com.avroserializer.SparkAvroExample target/scala-2.10/sparkavroserializer_2.10-1.0.jar","java.home":"/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre","java.version":"1.8.0_51","sun.io.unicode.encoding":"UnicodeBig"},"Classpath Entries":{"/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/spark-assembly-1.4.1-hadoop2.6.0.jar":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar":"System Classpath","http://192.168.2.68:52411/jars/sparkavroserializer_2.10-1.0.jar":"Added By User","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/conf/":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar":"System Classpath","/Users/quentin/Dev/spark-1.4.1-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"SparkAvroSerializer","App ID":"app-20151009152834-0003","Timestamp":1444397312569,"User":"quentin"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1444397318283,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"4\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"4\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1444397321361,"Executor ID":"0","Executor Info":{"Host":"192.168.2.68","Total Cores":1,"Log Urls":{"stdout":"http://192.168.2.68:8082/logPage/?appId=app-20151009152834-0003&executorId=0&logType=stdout","stderr":"http://192.168.2.68:8082/logPage/?appId=app-20151009152834-0003&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1444397321365,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1444397321510,"Executor ID":"1","Executor Info":{"Host":"192.168.2.68","Total Cores":1,"Log Urls":{"stdout":"http://192.168.2.68:8081/logPage/?appId=app-20151009152834-0003&executorId=1&logType=stdout","stderr":"http://192.168.2.68:8081/logPage/?appId=app-20151009152834-0003&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.2.68","Port":52425},"Maximum Memory":278019440,"Timestamp":1444397321640}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.2.68","Port":52427},"Maximum Memory":278019440,"Timestamp":1444397321806}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1444397321365,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397324075,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":1278,"Executor Run Time":1025,"Result Size":1573,"JVM GC Time":69,"Result Serialization Time":4,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586726,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"flatMap\"}","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397319073,"Completion Time":1444397324079,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1444397324085,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1444397324229,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[1],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1444397324250,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1444397324250,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397325521,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":717,"Executor Run Time":505,"Result Size":1754,"JVM GC Time":89,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":4,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"8\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397324249,"Completion Time":1444397325521,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1444397325522,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1444397325612,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[2],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1444397325627,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1444397325627,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397325800,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":51,"Executor Run Time":102,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599022,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"13\",\"name\":\"filter\"}","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"12\",\"name\":\"map\"}","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"10\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397325627,"Completion Time":1444397325800,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1444397325800,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":3,"Submission Time":1444397325883,"Stage Infos":[{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[3],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":0,"Attempt":0,"Launch Time":1444397325895,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":3,"Index":0,"Attempt":0,"Launch Time":1444397325895,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397326220,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":39,"Executor Run Time":250,"Result Size":1754,"JVM GC Time":24,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599022,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"21\",\"name\":\"map\"}","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"map\"}","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"18\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397325895,"Completion Time":1444397326220,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":3,"Completion Time":1444397326220,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":4,"Submission Time":1444397326326,"Stage Infos":[{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[4],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"30\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"30\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":4,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1444397326347,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":4,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1444397326347,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397326612,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":78,"Executor Run Time":166,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586764,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"28\",\"name\":\"map\"}","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"26\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"27\",\"name\":\"flatMap\"}","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397326347,"Completion Time":1444397326613,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":4,"Completion Time":1444397326613,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":5,"Submission Time":1444397326684,"Stage Infos":[{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[5],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":5,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":0,"Attempt":0,"Launch Time":1444397326705,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":5,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":5,"Index":0,"Attempt":0,"Launch Time":1444397326705,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397326814,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":67,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":17,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"34\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397326706,"Completion Time":1444397326815,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":5,"Completion Time":1444397326815,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":6,"Submission Time":1444397326881,"Stage Infos":[{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[6],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":6,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Launch Time":1444397326893,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":6,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Launch Time":1444397326893,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397327018,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":36,"Executor Run Time":77,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":21,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"39\",\"name\":\"filter\"}","Parent IDs":[20],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"37\",\"name\":\"map\"}","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":18,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"36\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":20,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"38\",\"name\":\"map\"}","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397326893,"Completion Time":1444397327019,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":6,"Completion Time":1444397327019,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":7,"Submission Time":1444397327083,"Stage Infos":[{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[7],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":7,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":0,"Attempt":0,"Launch Time":1444397327093,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":7,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":7,"Index":0,"Attempt":0,"Launch Time":1444397327093,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397327226,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":31,"Executor Run Time":85,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599060,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":25,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"47\",\"name\":\"map\"}","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":24,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"map\"}","Parent IDs":[23],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":22,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"44\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":23,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"45\",\"name\":\"map\"}","Parent IDs":[22],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397327093,"Completion Time":1444397327227,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":7,"Completion Time":1444397327227,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":8,"Submission Time":1444397327300,"Stage Infos":[{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[8],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"56\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"56\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":8,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1444397327331,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":8,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1444397327331,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397327791,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":153,"Executor Run Time":280,"Result Size":1573,"JVM GC Time":14,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586800,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"54\",\"name\":\"map\"}","Parent IDs":[27],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":27,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"53\",\"name\":\"flatMap\"}","Parent IDs":[26],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":26,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"52\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397327331,"Completion Time":1444397327791,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":8,"Completion Time":1444397327791,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":9,"Submission Time":1444397327855,"Stage Infos":[{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[9],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":9,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Launch Time":1444397327868,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":9,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Launch Time":1444397327868,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397327973,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":22,"Executor Run Time":69,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"60\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397327867,"Completion Time":1444397327973,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":9,"Completion Time":1444397327973,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":10,"Submission Time":1444397328084,"Stage Infos":[{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[10],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":10,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Launch Time":1444397328105,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":10,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Launch Time":1444397328105,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397328302,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":77,"Executor Run Time":101,"Result Size":1753,"JVM GC Time":13,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":34,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"65\",\"name\":\"filter\"}","Parent IDs":[33],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":33,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"64\",\"name\":\"map\"}","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"62\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":32,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"63\",\"name\":\"map\"}","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397328105,"Completion Time":1444397328302,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":10,"Completion Time":1444397328302,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":11,"Submission Time":1444397328359,"Stage Infos":[{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[11],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":11,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Launch Time":1444397328367,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":11,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Launch Time":1444397328367,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397328477,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":25,"Executor Run Time":70,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599096,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":38,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"73\",\"name\":\"map\"}","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"71\",\"name\":\"map\"}","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":35,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"70\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"72\",\"name\":\"map\"}","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397328367,"Completion Time":1444397328478,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":11,"Completion Time":1444397328478,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":12,"Submission Time":1444397328559,"Stage Infos":[{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[12],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"82\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"82\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":12,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1444397328585,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":12,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1444397328585,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397328822,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":98,"Executor Run Time":119,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586772,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":41,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"80\",\"name\":\"map\"}","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"79\",\"name\":\"flatMap\"}","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":39,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"78\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397328585,"Completion Time":1444397328823,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":12,"Completion Time":1444397328823,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":13,"Submission Time":1444397328901,"Stage Infos":[{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[13],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":13,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":0,"Attempt":0,"Launch Time":1444397328914,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":13,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":13,"Index":0,"Attempt":0,"Launch Time":1444397328914,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397329023,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":74,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599068,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"86\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397328914,"Completion Time":1444397329024,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":13,"Completion Time":1444397329024,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":14,"Submission Time":1444397329079,"Stage Infos":[{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[14],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":14,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":0,"Attempt":0,"Launch Time":1444397329089,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":14,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":14,"Index":0,"Attempt":0,"Launch Time":1444397329089,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397329198,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":25,"Executor Run Time":71,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599068,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":47,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"91\",\"name\":\"filter\"}","Parent IDs":[46],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":44,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"88\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":46,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"90\",\"name\":\"map\"}","Parent IDs":[45],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":45,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"89\",\"name\":\"map\"}","Parent IDs":[44],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397329089,"Completion Time":1444397329199,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":14,"Completion Time":1444397329199,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":15,"Submission Time":1444397329252,"Stage Infos":[{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[15],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":15,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":0,"Attempt":0,"Launch Time":1444397329266,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":15,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":15,"Index":0,"Attempt":0,"Launch Time":1444397329266,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397329366,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":65,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599068,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":51,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"99\",\"name\":\"map\"}","Parent IDs":[50],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":48,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"96\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":49,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"97\",\"name\":\"map\"}","Parent IDs":[48],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":50,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"98\",\"name\":\"map\"}","Parent IDs":[49],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397329266,"Completion Time":1444397329367,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":15,"Completion Time":1444397329367,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":16,"Submission Time":1444397329431,"Stage Infos":[{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[16],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"108\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"108\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":16,"Stage Attempt ID":0,"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1444397329455,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":16,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1444397329455,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397329697,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":64,"Executor Run Time":153,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586792,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":54,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"106\",\"name\":\"map\"}","Parent IDs":[53],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":53,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"105\",\"name\":\"flatMap\"}","Parent IDs":[52],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":52,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"104\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397329454,"Completion Time":1444397329697,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":16,"Completion Time":1444397329697,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":17,"Submission Time":1444397329769,"Stage Infos":[{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[17],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":17,"Stage Attempt ID":0,"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1444397329789,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":17,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1444397329789,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397329915,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":29,"Executor Run Time":84,"Result Size":1754,"JVM GC Time":7,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599088,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":56,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"112\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397329788,"Completion Time":1444397329916,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":17,"Completion Time":1444397329916,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":18,"Submission Time":1444397329968,"Stage Infos":[{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[18],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":18,"Stage Attempt ID":0,"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1444397329987,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":18,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1444397329987,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397330100,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":77,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599088,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":60,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"117\",\"name\":\"filter\"}","Parent IDs":[59],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":58,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"115\",\"name\":\"map\"}","Parent IDs":[57],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":57,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"114\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":59,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"116\",\"name\":\"map\"}","Parent IDs":[58],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397329987,"Completion Time":1444397330100,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":18,"Completion Time":1444397330100,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":19,"Submission Time":1444397330154,"Stage Infos":[{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[19],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":19,"Index":0,"Attempt":0,"Launch Time":1444397330166,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":19,"Index":0,"Attempt":0,"Launch Time":1444397330166,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397330280,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":77,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599088,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":64,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"125\",\"name\":\"map\"}","Parent IDs":[63],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":63,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"124\",\"name\":\"map\"}","Parent IDs":[62],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":62,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"123\",\"name\":\"map\"}","Parent IDs":[61],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":61,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"122\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397330166,"Completion Time":1444397330281,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":19,"Completion Time":1444397330281,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":20,"Submission Time":1444397330405,"Stage Infos":[{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[20],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"134\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"134\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":20,"Stage Attempt ID":0,"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1444397330430,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":20,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1444397330430,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397330726,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":116,"Executor Run Time":155,"Result Size":1573,"JVM GC Time":20,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586781,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":67,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"132\",\"name\":\"map\"}","Parent IDs":[66],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":65,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"130\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":66,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"131\",\"name\":\"flatMap\"}","Parent IDs":[65],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397330429,"Completion Time":1444397330727,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":20,"Completion Time":1444397330727,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":21,"Submission Time":1444397330783,"Stage Infos":[{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[21],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":21,"Stage Attempt ID":0,"Task Info":{"Task ID":21,"Index":0,"Attempt":0,"Launch Time":1444397330792,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":21,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":21,"Index":0,"Attempt":0,"Launch Time":1444397330792,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397330902,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":67,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599077,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":69,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"138\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397330791,"Completion Time":1444397330902,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":21,"Completion Time":1444397330903,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":22,"Submission Time":1444397330957,"Stage Infos":[{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[22],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":22,"Stage Attempt ID":0,"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1444397330966,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":22,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1444397330966,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397331083,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":29,"Executor Run Time":70,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599077,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":73,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"143\",\"name\":\"filter\"}","Parent IDs":[72],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":72,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"142\",\"name\":\"map\"}","Parent IDs":[71],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":70,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"140\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":71,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"141\",\"name\":\"map\"}","Parent IDs":[70],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397330966,"Completion Time":1444397331083,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":22,"Completion Time":1444397331083,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":23,"Submission Time":1444397331131,"Stage Infos":[{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[23],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":23,"Stage Attempt ID":0,"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Launch Time":1444397331146,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":23,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Launch Time":1444397331146,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397331231,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":57,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599077,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":77,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"151\",\"name\":\"map\"}","Parent IDs":[76],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":74,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"148\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":75,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"149\",\"name\":\"map\"}","Parent IDs":[74],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":76,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"150\",\"name\":\"map\"}","Parent IDs":[75],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397331146,"Completion Time":1444397331232,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":23,"Completion Time":1444397331232,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":24,"Submission Time":1444397331282,"Stage Infos":[{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[24],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"160\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"160\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1444397331301,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1444397331301,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397331487,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":45,"Executor Run Time":115,"Result Size":1573,"JVM GC Time":17,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586773,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":80,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"158\",\"name\":\"map\"}","Parent IDs":[79],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":79,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"157\",\"name\":\"flatMap\"}","Parent IDs":[78],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":78,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"156\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397331301,"Completion Time":1444397331487,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":24,"Completion Time":1444397331487,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":25,"Submission Time":1444397331538,"Stage Infos":[{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[25],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":25,"Stage Attempt ID":0,"Task Info":{"Task ID":25,"Index":0,"Attempt":0,"Launch Time":1444397331549,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":25,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":25,"Index":0,"Attempt":0,"Launch Time":1444397331549,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397331646,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":67,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":25,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":82,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"164\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397331548,"Completion Time":1444397331647,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":25,"Completion Time":1444397331647,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":26,"Submission Time":1444397331699,"Stage Infos":[{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[26],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":26,"Stage Attempt ID":0,"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1444397331709,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":26,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1444397331709,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397331796,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":18,"Executor Run Time":57,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":26,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":86,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"169\",\"name\":\"filter\"}","Parent IDs":[85],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":84,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"167\",\"name\":\"map\"}","Parent IDs":[83],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":85,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"168\",\"name\":\"map\"}","Parent IDs":[84],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":83,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"166\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397331709,"Completion Time":1444397331796,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":26,"Completion Time":1444397331797,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":27,"Submission Time":1444397331848,"Stage Infos":[{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[27],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":27,"Stage Attempt ID":0,"Task Info":{"Task ID":27,"Index":0,"Attempt":0,"Launch Time":1444397331866,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":27,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":27,"Index":0,"Attempt":0,"Launch Time":1444397331866,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397331944,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":47,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599069,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":27,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":90,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"177\",\"name\":\"map\"}","Parent IDs":[89],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":89,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"176\",\"name\":\"map\"}","Parent IDs":[88],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":87,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"174\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":88,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"175\",\"name\":\"map\"}","Parent IDs":[87],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397331866,"Completion Time":1444397331945,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":27,"Completion Time":1444397331945,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":28,"Submission Time":1444397331994,"Stage Infos":[{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[28],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"186\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"186\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":28,"Stage Attempt ID":0,"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1444397332011,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":28,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1444397332011,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397332192,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":48,"Executor Run Time":120,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586825,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":28,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":93,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"184\",\"name\":\"map\"}","Parent IDs":[92],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":92,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"183\",\"name\":\"flatMap\"}","Parent IDs":[91],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":91,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"182\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397332011,"Completion Time":1444397332192,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":28,"Completion Time":1444397332193,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":29,"Submission Time":1444397332251,"Stage Infos":[{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[29],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":29,"Stage Attempt ID":0,"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Launch Time":1444397332266,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":29,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Launch Time":1444397332266,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397332385,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":21,"Executor Run Time":58,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599121,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":29,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":95,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"190\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397332266,"Completion Time":1444397332385,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":29,"Completion Time":1444397332385,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":30,"Submission Time":1444397332464,"Stage Infos":[{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[30],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":30,"Stage Attempt ID":0,"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1444397332486,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":30,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1444397332486,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397332683,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":57,"Executor Run Time":126,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599121,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":30,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":99,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"195\",\"name\":\"filter\"}","Parent IDs":[98],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":97,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"193\",\"name\":\"map\"}","Parent IDs":[96],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":98,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"194\",\"name\":\"map\"}","Parent IDs":[97],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":96,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"192\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397332486,"Completion Time":1444397332684,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":30,"Completion Time":1444397332684,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":31,"Submission Time":1444397332734,"Stage Infos":[{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[31],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":31,"Stage Attempt ID":0,"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1444397332746,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":31,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1444397332746,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397332821,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":51,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599121,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":31,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":103,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"203\",\"name\":\"map\"}","Parent IDs":[102],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":101,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"201\",\"name\":\"map\"}","Parent IDs":[100],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":102,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"202\",\"name\":\"map\"}","Parent IDs":[101],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":100,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"200\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397332746,"Completion Time":1444397332822,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":31,"Completion Time":1444397332822,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":32,"Submission Time":1444397332872,"Stage Infos":[{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[32],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"212\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"212\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":32,"Stage Attempt ID":0,"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1444397332887,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":32,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1444397332887,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397333066,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":44,"Executor Run Time":122,"Result Size":1573,"JVM GC Time":13,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586791,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":32,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":106,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"210\",\"name\":\"map\"}","Parent IDs":[105],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":105,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"209\",\"name\":\"flatMap\"}","Parent IDs":[104],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":104,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"208\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397332887,"Completion Time":1444397333066,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":32,"Completion Time":1444397333067,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":33,"Submission Time":1444397333118,"Stage Infos":[{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[33],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":33,"Stage Attempt ID":0,"Task Info":{"Task ID":33,"Index":0,"Attempt":0,"Launch Time":1444397333127,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":33,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":33,"Index":0,"Attempt":0,"Launch Time":1444397333127,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397333222,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":69,"Result Size":1754,"JVM GC Time":17,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599087,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":33,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":108,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"216\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397333127,"Completion Time":1444397333223,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":33,"Completion Time":1444397333223,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":34,"Submission Time":1444397333275,"Stage Infos":[{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[34],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":34,"Stage Attempt ID":0,"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1444397333287,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":34,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1444397333287,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397333371,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":21,"Executor Run Time":54,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599087,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":34,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":112,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"221\",\"name\":\"filter\"}","Parent IDs":[111],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":111,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"220\",\"name\":\"map\"}","Parent IDs":[110],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":109,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"218\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":110,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"219\",\"name\":\"map\"}","Parent IDs":[109],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397333286,"Completion Time":1444397333372,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":34,"Completion Time":1444397333372,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":35,"Submission Time":1444397333419,"Stage Infos":[{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[35],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":35,"Stage Attempt ID":0,"Task Info":{"Task ID":35,"Index":0,"Attempt":0,"Launch Time":1444397333427,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":35,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":35,"Index":0,"Attempt":0,"Launch Time":1444397333427,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397333504,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":51,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599087,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":35,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":116,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"229\",\"name\":\"map\"}","Parent IDs":[115],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":115,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"228\",\"name\":\"map\"}","Parent IDs":[114],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":114,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"227\",\"name\":\"map\"}","Parent IDs":[113],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":113,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"226\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397333427,"Completion Time":1444397333505,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":35,"Completion Time":1444397333505,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":36,"Submission Time":1444397333553,"Stage Infos":[{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[36],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"238\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"238\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":36,"Stage Attempt ID":0,"Task Info":{"Task ID":36,"Index":0,"Attempt":0,"Launch Time":1444397333580,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":36,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":36,"Index":0,"Attempt":0,"Launch Time":1444397333580,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397333808,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":105,"Executor Run Time":109,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586762,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":36,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":119,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"236\",\"name\":\"map\"}","Parent IDs":[118],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":117,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"234\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":118,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"235\",\"name\":\"flatMap\"}","Parent IDs":[117],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397333580,"Completion Time":1444397333809,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":36,"Completion Time":1444397333809,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":37,"Submission Time":1444397333857,"Stage Infos":[{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[37],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":37,"Stage Attempt ID":0,"Task Info":{"Task ID":37,"Index":0,"Attempt":0,"Launch Time":1444397333866,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":37,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":37,"Index":0,"Attempt":0,"Launch Time":1444397333866,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397333941,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":50,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599058,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":37,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":121,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"242\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397333865,"Completion Time":1444397333941,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":37,"Completion Time":1444397333941,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":38,"Submission Time":1444397333997,"Stage Infos":[{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[38],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":38,"Stage Attempt ID":0,"Task Info":{"Task ID":38,"Index":0,"Attempt":0,"Launch Time":1444397334006,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":38,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":38,"Index":0,"Attempt":0,"Launch Time":1444397334006,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397334080,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":46,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599058,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":38,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":125,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"247\",\"name\":\"filter\"}","Parent IDs":[124],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":122,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"244\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":124,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"246\",\"name\":\"map\"}","Parent IDs":[123],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":123,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"245\",\"name\":\"map\"}","Parent IDs":[122],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397334006,"Completion Time":1444397334081,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":38,"Completion Time":1444397334081,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":39,"Submission Time":1444397334129,"Stage Infos":[{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[39],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":39,"Stage Attempt ID":0,"Task Info":{"Task ID":39,"Index":0,"Attempt":0,"Launch Time":1444397334146,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":39,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":39,"Index":0,"Attempt":0,"Launch Time":1444397334146,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397334240,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":64,"Result Size":1754,"JVM GC Time":17,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599058,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":39,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":129,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"255\",\"name\":\"map\"}","Parent IDs":[128],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":127,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"253\",\"name\":\"map\"}","Parent IDs":[126],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":126,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"252\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":128,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"254\",\"name\":\"map\"}","Parent IDs":[127],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397334145,"Completion Time":1444397334241,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":39,"Completion Time":1444397334241,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":40,"Submission Time":1444397334287,"Stage Infos":[{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[40],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"264\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"264\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":40,"Stage Attempt ID":0,"Task Info":{"Task ID":40,"Index":0,"Attempt":0,"Launch Time":1444397334306,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":40,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":40,"Index":0,"Attempt":0,"Launch Time":1444397334306,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397334450,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":39,"Executor Run Time":87,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586733,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":40,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":132,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"262\",\"name\":\"map\"}","Parent IDs":[131],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":130,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"260\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":131,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"261\",\"name\":\"flatMap\"}","Parent IDs":[130],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397334306,"Completion Time":1444397334451,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":40,"Completion Time":1444397334451,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":41,"Submission Time":1444397334501,"Stage Infos":[{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[41],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":41,"Stage Attempt ID":0,"Task Info":{"Task ID":41,"Index":0,"Attempt":0,"Launch Time":1444397334507,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":41,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":41,"Index":0,"Attempt":0,"Launch Time":1444397334507,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397334585,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":50,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599029,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":41,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":134,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"268\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397334507,"Completion Time":1444397334585,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":41,"Completion Time":1444397334585,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":42,"Submission Time":1444397334637,"Stage Infos":[{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[42],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":42,"Stage Attempt ID":0,"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Launch Time":1444397334647,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":42,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Launch Time":1444397334647,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397334714,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":43,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599029,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":42,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":138,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"273\",\"name\":\"filter\"}","Parent IDs":[137],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":135,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"270\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":136,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"271\",\"name\":\"map\"}","Parent IDs":[135],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":137,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"272\",\"name\":\"map\"}","Parent IDs":[136],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397334646,"Completion Time":1444397334715,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":42,"Completion Time":1444397334715,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":43,"Submission Time":1444397334792,"Stage Infos":[{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[43],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":43,"Stage Attempt ID":0,"Task Info":{"Task ID":43,"Index":0,"Attempt":0,"Launch Time":1444397334826,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":43,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":43,"Index":0,"Attempt":0,"Launch Time":1444397334826,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397334911,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":57,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599029,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":43,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":142,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"281\",\"name\":\"map\"}","Parent IDs":[141],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":140,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"279\",\"name\":\"map\"}","Parent IDs":[139],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":141,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"280\",\"name\":\"map\"}","Parent IDs":[140],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":139,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"278\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397334825,"Completion Time":1444397334911,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":43,"Completion Time":1444397334912,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":44,"Submission Time":1444397334955,"Stage Infos":[{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[44],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"290\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"290\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":44,"Stage Attempt ID":0,"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Launch Time":1444397334971,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":44,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Launch Time":1444397334971,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397335169,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":47,"Executor Run Time":129,"Result Size":1573,"JVM GC Time":19,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586763,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":44,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":145,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"288\",\"name\":\"map\"}","Parent IDs":[144],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":143,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"286\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":144,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"287\",\"name\":\"flatMap\"}","Parent IDs":[143],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397334970,"Completion Time":1444397335170,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":44,"Completion Time":1444397335170,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":45,"Submission Time":1444397335224,"Stage Infos":[{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[45],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":45,"Stage Attempt ID":0,"Task Info":{"Task ID":45,"Index":0,"Attempt":0,"Launch Time":1444397335230,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":45,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":45,"Index":0,"Attempt":0,"Launch Time":1444397335230,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397335296,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":41,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599059,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":45,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":147,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"294\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397335230,"Completion Time":1444397335296,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":45,"Completion Time":1444397335296,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":46,"Submission Time":1444397335348,"Stage Infos":[{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[46],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":46,"Stage Attempt ID":0,"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Launch Time":1444397335366,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":46,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Launch Time":1444397335366,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397335434,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":46,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599059,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":46,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":151,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"299\",\"name\":\"filter\"}","Parent IDs":[150],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":150,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"298\",\"name\":\"map\"}","Parent IDs":[149],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":149,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"297\",\"name\":\"map\"}","Parent IDs":[148],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":148,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"296\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397335366,"Completion Time":1444397335435,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":46,"Completion Time":1444397335435,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":47,"Submission Time":1444397335483,"Stage Infos":[{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[47],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":47,"Stage Attempt ID":0,"Task Info":{"Task ID":47,"Index":0,"Attempt":0,"Launch Time":1444397335488,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":47,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":47,"Index":0,"Attempt":0,"Launch Time":1444397335488,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397335570,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":58,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599059,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":47,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":155,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"307\",\"name\":\"map\"}","Parent IDs":[154],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":152,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"304\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":154,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"306\",\"name\":\"map\"}","Parent IDs":[153],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":153,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"305\",\"name\":\"map\"}","Parent IDs":[152],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397335488,"Completion Time":1444397335570,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":47,"Completion Time":1444397335570,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":48,"Submission Time":1444397335618,"Stage Infos":[{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[48],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"316\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"316\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":48,"Stage Attempt ID":0,"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Launch Time":1444397335633,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":48,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Launch Time":1444397335633,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397335821,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":44,"Executor Run Time":125,"Result Size":1573,"JVM GC Time":19,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586801,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":48,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":158,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"314\",\"name\":\"map\"}","Parent IDs":[157],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":157,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"313\",\"name\":\"flatMap\"}","Parent IDs":[156],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":156,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"312\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397335632,"Completion Time":1444397335822,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":48,"Completion Time":1444397335822,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":49,"Submission Time":1444397335870,"Stage Infos":[{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[49],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":49,"Stage Attempt ID":0,"Task Info":{"Task ID":49,"Index":0,"Attempt":0,"Launch Time":1444397335887,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":49,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":49,"Index":0,"Attempt":0,"Launch Time":1444397335887,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397335961,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":51,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599097,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":49,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":160,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"320\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397335887,"Completion Time":1444397335962,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":49,"Completion Time":1444397335962,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":50,"Submission Time":1444397336055,"Stage Infos":[{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[50],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":50,"Stage Attempt ID":0,"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Launch Time":1444397336085,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":50,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Launch Time":1444397336085,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397336183,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":43,"Executor Run Time":46,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599097,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":50,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":164,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"325\",\"name\":\"filter\"}","Parent IDs":[163],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":161,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"322\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":163,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"324\",\"name\":\"map\"}","Parent IDs":[162],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":162,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"323\",\"name\":\"map\"}","Parent IDs":[161],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397336085,"Completion Time":1444397336184,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":50,"Completion Time":1444397336184,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":51,"Submission Time":1444397336230,"Stage Infos":[{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[51],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":51,"Stage Attempt ID":0,"Task Info":{"Task ID":51,"Index":0,"Attempt":0,"Launch Time":1444397336247,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":51,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":51,"Index":0,"Attempt":0,"Launch Time":1444397336247,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397336315,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":42,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599097,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":51,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":168,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"333\",\"name\":\"map\"}","Parent IDs":[167],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":167,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"332\",\"name\":\"map\"}","Parent IDs":[166],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":166,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"331\",\"name\":\"map\"}","Parent IDs":[165],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":165,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"330\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397336247,"Completion Time":1444397336316,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":51,"Completion Time":1444397336316,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":52,"Submission Time":1444397336356,"Stage Infos":[{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[52],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"342\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"342\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":52,"Stage Attempt ID":0,"Task Info":{"Task ID":52,"Index":0,"Attempt":0,"Launch Time":1444397336369,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":52,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":52,"Index":0,"Attempt":0,"Launch Time":1444397336369,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397336533,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":35,"Executor Run Time":107,"Result Size":1573,"JVM GC Time":19,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586768,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":52,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":171,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"340\",\"name\":\"map\"}","Parent IDs":[170],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":169,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"338\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":170,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"339\",\"name\":\"flatMap\"}","Parent IDs":[169],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397336369,"Completion Time":1444397336534,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":52,"Completion Time":1444397336534,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":53,"Submission Time":1444397336584,"Stage Infos":[{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[53],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":53,"Stage Attempt ID":0,"Task Info":{"Task ID":53,"Index":0,"Attempt":0,"Launch Time":1444397336590,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":53,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":53,"Index":0,"Attempt":0,"Launch Time":1444397336590,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397336668,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":17,"Executor Run Time":52,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599064,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":53,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":173,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"346\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397336590,"Completion Time":1444397336668,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":53,"Completion Time":1444397336668,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":54,"Submission Time":1444397336717,"Stage Infos":[{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[54],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":54,"Stage Attempt ID":0,"Task Info":{"Task ID":54,"Index":0,"Attempt":0,"Launch Time":1444397336727,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":54,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":54,"Index":0,"Attempt":0,"Launch Time":1444397336727,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397336793,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":16,"Executor Run Time":41,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599064,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":54,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":177,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"351\",\"name\":\"filter\"}","Parent IDs":[176],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":174,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"348\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":176,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"350\",\"name\":\"map\"}","Parent IDs":[175],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":175,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"349\",\"name\":\"map\"}","Parent IDs":[174],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397336727,"Completion Time":1444397336793,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":54,"Completion Time":1444397336793,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":55,"Submission Time":1444397336837,"Stage Infos":[{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[55],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":55,"Stage Attempt ID":0,"Task Info":{"Task ID":55,"Index":0,"Attempt":0,"Launch Time":1444397336847,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":55,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":55,"Index":0,"Attempt":0,"Launch Time":1444397336847,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397336915,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":46,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599064,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":55,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":181,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"359\",\"name\":\"map\"}","Parent IDs":[180],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":180,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"358\",\"name\":\"map\"}","Parent IDs":[179],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":179,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"357\",\"name\":\"map\"}","Parent IDs":[178],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":178,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"356\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397336846,"Completion Time":1444397336915,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":55,"Completion Time":1444397336916,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":56,"Submission Time":1444397336962,"Stage Infos":[{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[56],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"368\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"368\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":56,"Stage Attempt ID":0,"Task Info":{"Task ID":56,"Index":0,"Attempt":0,"Launch Time":1444397336980,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":56,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":56,"Index":0,"Attempt":0,"Launch Time":1444397336980,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397337150,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":42,"Executor Run Time":113,"Result Size":1573,"JVM GC Time":9,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586758,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":56,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":184,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"366\",\"name\":\"map\"}","Parent IDs":[183],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":182,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"364\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":183,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"365\",\"name\":\"flatMap\"}","Parent IDs":[182],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397336980,"Completion Time":1444397337150,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":56,"Completion Time":1444397337151,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":57,"Submission Time":1444397337276,"Stage Infos":[{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[57],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":57,"Stage Attempt ID":0,"Task Info":{"Task ID":57,"Index":0,"Attempt":0,"Launch Time":1444397337305,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":57,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":57,"Index":0,"Attempt":0,"Launch Time":1444397337305,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397337369,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":42,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599054,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":57,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":186,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"372\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397337305,"Completion Time":1444397337370,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":57,"Completion Time":1444397337370,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":58,"Submission Time":1444397337417,"Stage Infos":[{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[58],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":58,"Stage Attempt ID":0,"Task Info":{"Task ID":58,"Index":0,"Attempt":0,"Launch Time":1444397337426,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":58,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":58,"Index":0,"Attempt":0,"Launch Time":1444397337426,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397337497,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":44,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599054,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":58,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":190,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"377\",\"name\":\"filter\"}","Parent IDs":[189],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":188,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"375\",\"name\":\"map\"}","Parent IDs":[187],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":187,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"374\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":189,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"376\",\"name\":\"map\"}","Parent IDs":[188],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397337426,"Completion Time":1444397337498,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":58,"Completion Time":1444397337498,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":59,"Submission Time":1444397337542,"Stage Infos":[{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[59],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":59,"Stage Attempt ID":0,"Task Info":{"Task ID":59,"Index":0,"Attempt":0,"Launch Time":1444397337550,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":59,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":59,"Index":0,"Attempt":0,"Launch Time":1444397337550,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397337617,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":44,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599054,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":59,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":194,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"385\",\"name\":\"map\"}","Parent IDs":[193],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":191,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"382\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":193,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"384\",\"name\":\"map\"}","Parent IDs":[192],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":192,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"383\",\"name\":\"map\"}","Parent IDs":[191],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397337550,"Completion Time":1444397337617,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":59,"Completion Time":1444397337617,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":60,"Submission Time":1444397337658,"Stage Infos":[{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[60],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"394\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"394\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":60,"Stage Attempt ID":0,"Task Info":{"Task ID":60,"Index":0,"Attempt":0,"Launch Time":1444397337672,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":60,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":60,"Index":0,"Attempt":0,"Launch Time":1444397337672,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397337833,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":39,"Executor Run Time":102,"Result Size":1573,"JVM GC Time":18,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586794,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":60,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":197,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"392\",\"name\":\"map\"}","Parent IDs":[196],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":196,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"391\",\"name\":\"flatMap\"}","Parent IDs":[195],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":195,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"390\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397337672,"Completion Time":1444397337833,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":60,"Completion Time":1444397337834,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":61,"Submission Time":1444397337884,"Stage Infos":[{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[61],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":61,"Stage Attempt ID":0,"Task Info":{"Task ID":61,"Index":0,"Attempt":0,"Launch Time":1444397337889,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":61,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":61,"Index":0,"Attempt":0,"Launch Time":1444397337889,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397337954,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":45,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599090,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":61,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":199,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"398\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397337888,"Completion Time":1444397337955,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":61,"Completion Time":1444397337955,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":62,"Submission Time":1444397337995,"Stage Infos":[{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[62],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":62,"Stage Attempt ID":0,"Task Info":{"Task ID":62,"Index":0,"Attempt":0,"Launch Time":1444397338006,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":62,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":62,"Index":0,"Attempt":0,"Launch Time":1444397338006,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397338071,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":44,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599090,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":62,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":203,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"403\",\"name\":\"filter\"}","Parent IDs":[202],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":201,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"401\",\"name\":\"map\"}","Parent IDs":[200],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":200,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"400\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":202,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"402\",\"name\":\"map\"}","Parent IDs":[201],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397338006,"Completion Time":1444397338071,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":62,"Completion Time":1444397338071,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":63,"Submission Time":1444397338116,"Stage Infos":[{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[63],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":63,"Stage Attempt ID":0,"Task Info":{"Task ID":63,"Index":0,"Attempt":0,"Launch Time":1444397338126,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":63,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":63,"Index":0,"Attempt":0,"Launch Time":1444397338126,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397338194,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":45,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599090,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":63,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":207,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"411\",\"name\":\"map\"}","Parent IDs":[206],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":206,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"410\",\"name\":\"map\"}","Parent IDs":[205],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":205,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"409\",\"name\":\"map\"}","Parent IDs":[204],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":204,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"408\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397338126,"Completion Time":1444397338194,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":63,"Completion Time":1444397338194,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":64,"Submission Time":1444397338239,"Stage Infos":[{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[64],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"420\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"420\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":64,"Stage Attempt ID":0,"Task Info":{"Task ID":64,"Index":0,"Attempt":0,"Launch Time":1444397338308,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":64,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":64,"Index":0,"Attempt":0,"Launch Time":1444397338308,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397338641,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":109,"Executor Run Time":204,"Result Size":1573,"JVM GC Time":17,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586782,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":64,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":210,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"418\",\"name\":\"map\"}","Parent IDs":[209],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":209,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"417\",\"name\":\"flatMap\"}","Parent IDs":[208],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":208,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"416\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397338308,"Completion Time":1444397338641,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":64,"Completion Time":1444397338641,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":65,"Submission Time":1444397338702,"Stage Infos":[{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[65],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":65,"Stage Attempt ID":0,"Task Info":{"Task ID":65,"Index":0,"Attempt":0,"Launch Time":1444397338707,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":65,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":65,"Index":0,"Attempt":0,"Launch Time":1444397338707,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397338790,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":19,"Executor Run Time":56,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":65,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":212,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"424\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397338707,"Completion Time":1444397338791,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":65,"Completion Time":1444397338791,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":66,"Submission Time":1444397338836,"Stage Infos":[{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[66],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":66,"Stage Attempt ID":0,"Task Info":{"Task ID":66,"Index":0,"Attempt":0,"Launch Time":1444397338847,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":66,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":66,"Index":0,"Attempt":0,"Launch Time":1444397338847,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397338909,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":43,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":66,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":216,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"429\",\"name\":\"filter\"}","Parent IDs":[215],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":214,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"427\",\"name\":\"map\"}","Parent IDs":[213],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":213,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"426\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":215,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"428\",\"name\":\"map\"}","Parent IDs":[214],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397338847,"Completion Time":1444397338909,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":66,"Completion Time":1444397338909,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":67,"Submission Time":1444397338956,"Stage Infos":[{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[67],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":67,"Stage Attempt ID":0,"Task Info":{"Task ID":67,"Index":0,"Attempt":0,"Launch Time":1444397338966,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":67,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":67,"Index":0,"Attempt":0,"Launch Time":1444397338966,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397339029,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":40,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599078,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":67,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":220,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"437\",\"name\":\"map\"}","Parent IDs":[219],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":219,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"436\",\"name\":\"map\"}","Parent IDs":[218],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":218,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"435\",\"name\":\"map\"}","Parent IDs":[217],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":217,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"434\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397338966,"Completion Time":1444397339030,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":67,"Completion Time":1444397339030,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":68,"Submission Time":1444397339073,"Stage Infos":[{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[68],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"446\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"446\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":68,"Stage Attempt ID":0,"Task Info":{"Task ID":68,"Index":0,"Attempt":0,"Launch Time":1444397339088,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":68,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":68,"Index":0,"Attempt":0,"Launch Time":1444397339088,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397339238,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":38,"Executor Run Time":102,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586770,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":68,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":223,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"444\",\"name\":\"map\"}","Parent IDs":[222],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":222,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"443\",\"name\":\"flatMap\"}","Parent IDs":[221],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":221,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"442\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397339088,"Completion Time":1444397339238,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":68,"Completion Time":1444397339238,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":69,"Submission Time":1444397339290,"Stage Infos":[{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[69],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":69,"Stage Attempt ID":0,"Task Info":{"Task ID":69,"Index":0,"Attempt":0,"Launch Time":1444397339306,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":69,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":69,"Index":0,"Attempt":0,"Launch Time":1444397339306,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397339371,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":43,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":69,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":225,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"450\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397339306,"Completion Time":1444397339371,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":69,"Completion Time":1444397339371,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":70,"Submission Time":1444397339419,"Stage Infos":[{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[70],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":70,"Stage Attempt ID":0,"Task Info":{"Task ID":70,"Index":0,"Attempt":0,"Launch Time":1444397339426,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":70,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":70,"Index":0,"Attempt":0,"Launch Time":1444397339426,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397339522,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":73,"Result Size":1753,"JVM GC Time":30,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":70,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":229,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"455\",\"name\":\"filter\"}","Parent IDs":[228],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":226,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"452\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":228,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"454\",\"name\":\"map\"}","Parent IDs":[227],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":227,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"453\",\"name\":\"map\"}","Parent IDs":[226],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397339426,"Completion Time":1444397339522,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":70,"Completion Time":1444397339522,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":71,"Submission Time":1444397339569,"Stage Infos":[{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[71],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":71,"Stage Attempt ID":0,"Task Info":{"Task ID":71,"Index":0,"Attempt":0,"Launch Time":1444397339590,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":71,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":71,"Index":0,"Attempt":0,"Launch Time":1444397339590,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397339676,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":25,"Executor Run Time":52,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599066,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":71,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":233,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"463\",\"name\":\"map\"}","Parent IDs":[232],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":232,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"462\",\"name\":\"map\"}","Parent IDs":[231],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":230,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"460\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":231,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"461\",\"name\":\"map\"}","Parent IDs":[230],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397339590,"Completion Time":1444397339676,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":71,"Completion Time":1444397339676,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":72,"Submission Time":1444397339944,"Stage Infos":[{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[72],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"472\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"472\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":72,"Stage Attempt ID":0,"Task Info":{"Task ID":72,"Index":0,"Attempt":0,"Launch Time":1444397339965,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":72,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":72,"Index":0,"Attempt":0,"Launch Time":1444397339965,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397340253,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":42,"Executor Run Time":234,"Result Size":1573,"JVM GC Time":8,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586803,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":72,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":236,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"470\",\"name\":\"map\"}","Parent IDs":[235],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":234,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"468\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":235,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"469\",\"name\":\"flatMap\"}","Parent IDs":[234],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397339964,"Completion Time":1444397340254,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":72,"Completion Time":1444397340254,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":73,"Submission Time":1444397340304,"Stage Infos":[{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[73],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":73,"Stage Attempt ID":0,"Task Info":{"Task ID":73,"Index":0,"Attempt":0,"Launch Time":1444397340325,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":73,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":73,"Index":0,"Attempt":0,"Launch Time":1444397340325,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397340385,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":35,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599099,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":73,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":238,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"476\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397340324,"Completion Time":1444397340385,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":73,"Completion Time":1444397340385,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":74,"Submission Time":1444397340439,"Stage Infos":[{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[74],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":74,"Stage Attempt ID":0,"Task Info":{"Task ID":74,"Index":0,"Attempt":0,"Launch Time":1444397340448,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":74,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":74,"Index":0,"Attempt":0,"Launch Time":1444397340448,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397340509,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":39,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599099,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":74,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":242,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"481\",\"name\":\"filter\"}","Parent IDs":[241],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":240,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"479\",\"name\":\"map\"}","Parent IDs":[239],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":241,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"480\",\"name\":\"map\"}","Parent IDs":[240],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":239,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"478\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397340447,"Completion Time":1444397340510,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":74,"Completion Time":1444397340510,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":75,"Submission Time":1444397340555,"Stage Infos":[{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[75],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":75,"Stage Attempt ID":0,"Task Info":{"Task ID":75,"Index":0,"Attempt":0,"Launch Time":1444397340566,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":75,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":75,"Index":0,"Attempt":0,"Launch Time":1444397340566,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397340624,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":37,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599099,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":75,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":246,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"489\",\"name\":\"map\"}","Parent IDs":[245],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":244,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"487\",\"name\":\"map\"}","Parent IDs":[243],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":243,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"486\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":245,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"488\",\"name\":\"map\"}","Parent IDs":[244],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397340566,"Completion Time":1444397340624,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":75,"Completion Time":1444397340624,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":76,"Submission Time":1444397340670,"Stage Infos":[{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[76],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"498\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"498\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":76,"Stage Attempt ID":0,"Task Info":{"Task ID":76,"Index":0,"Attempt":0,"Launch Time":1444397340688,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":76,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":76,"Index":0,"Attempt":0,"Launch Time":1444397340688,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397340887,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":61,"Executor Run Time":128,"Result Size":1573,"JVM GC Time":6,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586755,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":76,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":249,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"496\",\"name\":\"map\"}","Parent IDs":[248],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":247,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"494\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":248,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"495\",\"name\":\"flatMap\"}","Parent IDs":[247],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397340688,"Completion Time":1444397340887,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":76,"Completion Time":1444397340887,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":77,"Submission Time":1444397340937,"Stage Infos":[{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[77],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":77,"Stage Attempt ID":0,"Task Info":{"Task ID":77,"Index":0,"Attempt":0,"Launch Time":1444397340948,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":77,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":77,"Index":0,"Attempt":0,"Launch Time":1444397340948,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397341007,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":11,"Executor Run Time":41,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599051,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":77,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":251,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"502\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397340948,"Completion Time":1444397341007,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":77,"Completion Time":1444397341007,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":78,"Submission Time":1444397341056,"Stage Infos":[{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[78],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":78,"Stage Attempt ID":0,"Task Info":{"Task ID":78,"Index":0,"Attempt":0,"Launch Time":1444397341067,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":78,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":78,"Index":0,"Attempt":0,"Launch Time":1444397341067,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397341124,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":36,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599051,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":78,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":255,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"507\",\"name\":\"filter\"}","Parent IDs":[254],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":253,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"505\",\"name\":\"map\"}","Parent IDs":[252],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":252,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"504\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":254,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"506\",\"name\":\"map\"}","Parent IDs":[253],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397341066,"Completion Time":1444397341124,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":78,"Completion Time":1444397341124,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":79,"Submission Time":1444397341168,"Stage Infos":[{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[79],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":79,"Stage Attempt ID":0,"Task Info":{"Task ID":79,"Index":0,"Attempt":0,"Launch Time":1444397341200,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":79,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":79,"Index":0,"Attempt":0,"Launch Time":1444397341200,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397341300,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":23,"Executor Run Time":70,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599051,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":79,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":259,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"515\",\"name\":\"map\"}","Parent IDs":[258],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":258,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"514\",\"name\":\"map\"}","Parent IDs":[257],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":256,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"512\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":257,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"513\",\"name\":\"map\"}","Parent IDs":[256],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397341199,"Completion Time":1444397341300,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":79,"Completion Time":1444397341301,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":80,"Submission Time":1444397341343,"Stage Infos":[{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[80],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"524\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"524\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":80,"Stage Attempt ID":0,"Task Info":{"Task ID":80,"Index":0,"Attempt":0,"Launch Time":1444397341358,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":80,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":80,"Index":0,"Attempt":0,"Launch Time":1444397341358,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397341507,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":40,"Executor Run Time":95,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586785,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":80,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":262,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"522\",\"name\":\"map\"}","Parent IDs":[261],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":261,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"521\",\"name\":\"flatMap\"}","Parent IDs":[260],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":260,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"520\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397341357,"Completion Time":1444397341507,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":80,"Completion Time":1444397341508,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":81,"Submission Time":1444397341556,"Stage Infos":[{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[81],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":81,"Stage Attempt ID":0,"Task Info":{"Task ID":81,"Index":0,"Attempt":0,"Launch Time":1444397341567,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":81,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":81,"Index":0,"Attempt":0,"Launch Time":1444397341567,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397341635,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":46,"Result Size":1754,"JVM GC Time":5,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599081,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":81,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":264,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"528\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397341566,"Completion Time":1444397341635,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":81,"Completion Time":1444397341635,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":82,"Submission Time":1444397341678,"Stage Infos":[{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[82],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":82,"Stage Attempt ID":0,"Task Info":{"Task ID":82,"Index":0,"Attempt":0,"Launch Time":1444397341688,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":82,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":82,"Index":0,"Attempt":0,"Launch Time":1444397341688,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397341741,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":35,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599081,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":82,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":268,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"533\",\"name\":\"filter\"}","Parent IDs":[267],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":265,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"530\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":267,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"532\",\"name\":\"map\"}","Parent IDs":[266],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":266,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"531\",\"name\":\"map\"}","Parent IDs":[265],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397341688,"Completion Time":1444397341742,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":82,"Completion Time":1444397341742,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":83,"Submission Time":1444397341786,"Stage Infos":[{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[83],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":83,"Stage Attempt ID":0,"Task Info":{"Task ID":83,"Index":0,"Attempt":0,"Launch Time":1444397341805,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":83,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":83,"Index":0,"Attempt":0,"Launch Time":1444397341805,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397341863,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599081,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":83,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":272,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"541\",\"name\":\"map\"}","Parent IDs":[271],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":271,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"540\",\"name\":\"map\"}","Parent IDs":[270],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":269,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"538\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":270,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"539\",\"name\":\"map\"}","Parent IDs":[269],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397341805,"Completion Time":1444397341863,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":83,"Completion Time":1444397341864,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":84,"Submission Time":1444397341907,"Stage Infos":[{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[84],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"550\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"550\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":84,"Stage Attempt ID":0,"Task Info":{"Task ID":84,"Index":0,"Attempt":0,"Launch Time":1444397341926,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":84,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":84,"Index":0,"Attempt":0,"Launch Time":1444397341926,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397342091,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":34,"Executor Run Time":123,"Result Size":1573,"JVM GC Time":18,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586767,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":84,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":275,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"548\",\"name\":\"map\"}","Parent IDs":[274],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":274,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"547\",\"name\":\"flatMap\"}","Parent IDs":[273],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":273,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"546\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397341925,"Completion Time":1444397342093,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":84,"Completion Time":1444397342093,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":85,"Submission Time":1444397342142,"Stage Infos":[{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[85],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":85,"Stage Attempt ID":0,"Task Info":{"Task ID":85,"Index":0,"Attempt":0,"Launch Time":1444397342150,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":85,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":85,"Index":0,"Attempt":0,"Launch Time":1444397342150,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397342212,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":11,"Executor Run Time":43,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599063,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":85,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":277,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"554\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397342150,"Completion Time":1444397342213,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":85,"Completion Time":1444397342213,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":86,"Submission Time":1444397342257,"Stage Infos":[{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[86],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":86,"Stage Attempt ID":0,"Task Info":{"Task ID":86,"Index":0,"Attempt":0,"Launch Time":1444397342267,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":86,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":86,"Index":0,"Attempt":0,"Launch Time":1444397342267,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397342325,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":37,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599063,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":86,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":281,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"559\",\"name\":\"filter\"}","Parent IDs":[280],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":280,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"558\",\"name\":\"map\"}","Parent IDs":[279],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":279,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"557\",\"name\":\"map\"}","Parent IDs":[278],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":278,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"556\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397342266,"Completion Time":1444397342326,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":86,"Completion Time":1444397342326,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":87,"Submission Time":1444397342425,"Stage Infos":[{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[87],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":87,"Stage Attempt ID":0,"Task Info":{"Task ID":87,"Index":0,"Attempt":0,"Launch Time":1444397342449,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":87,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":87,"Index":0,"Attempt":0,"Launch Time":1444397342449,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397342519,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":24,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599063,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":87,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":285,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"567\",\"name\":\"map\"}","Parent IDs":[284],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":284,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"566\",\"name\":\"map\"}","Parent IDs":[283],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":282,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"564\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":283,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"565\",\"name\":\"map\"}","Parent IDs":[282],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397342448,"Completion Time":1444397342520,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":87,"Completion Time":1444397342520,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":88,"Submission Time":1444397342566,"Stage Infos":[{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[88],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"576\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"576\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":88,"Stage Attempt ID":0,"Task Info":{"Task ID":88,"Index":0,"Attempt":0,"Launch Time":1444397342585,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":88,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":88,"Index":0,"Attempt":0,"Launch Time":1444397342585,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397342741,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":41,"Executor Run Time":106,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586766,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":88,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":288,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"574\",\"name\":\"map\"}","Parent IDs":[287],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":286,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"572\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":287,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"573\",\"name\":\"flatMap\"}","Parent IDs":[286],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397342584,"Completion Time":1444397342741,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":88,"Completion Time":1444397342741,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":89,"Submission Time":1444397342796,"Stage Infos":[{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[89],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":89,"Stage Attempt ID":0,"Task Info":{"Task ID":89,"Index":0,"Attempt":0,"Launch Time":1444397342807,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":89,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":89,"Index":0,"Attempt":0,"Launch Time":1444397342807,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397342867,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":15,"Executor Run Time":37,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599062,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":89,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":290,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"580\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397342807,"Completion Time":1444397342867,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":89,"Completion Time":1444397342867,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":90,"Submission Time":1444397342917,"Stage Infos":[{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[90],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":90,"Stage Attempt ID":0,"Task Info":{"Task ID":90,"Index":0,"Attempt":0,"Launch Time":1444397342926,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":90,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":90,"Index":0,"Attempt":0,"Launch Time":1444397342926,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397342994,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":14,"Executor Run Time":48,"Result Size":1753,"JVM GC Time":10,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599062,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":90,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":294,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"585\",\"name\":\"filter\"}","Parent IDs":[293],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":293,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"584\",\"name\":\"map\"}","Parent IDs":[292],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":291,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"582\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":292,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"583\",\"name\":\"map\"}","Parent IDs":[291],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397342926,"Completion Time":1444397342995,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":90,"Completion Time":1444397342995,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":91,"Submission Time":1444397343039,"Stage Infos":[{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[91],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":91,"Stage Attempt ID":0,"Task Info":{"Task ID":91,"Index":0,"Attempt":0,"Launch Time":1444397343048,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":91,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":91,"Index":0,"Attempt":0,"Launch Time":1444397343048,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397343104,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":38,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599062,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":91,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":298,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"593\",\"name\":\"map\"}","Parent IDs":[297],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":297,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"592\",\"name\":\"map\"}","Parent IDs":[296],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":296,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"591\",\"name\":\"map\"}","Parent IDs":[295],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":295,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"590\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397343047,"Completion Time":1444397343105,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":91,"Completion Time":1444397343105,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":92,"Submission Time":1444397343146,"Stage Infos":[{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[92],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"602\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"602\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":92,"Stage Attempt ID":0,"Task Info":{"Task ID":92,"Index":0,"Attempt":0,"Launch Time":1444397343165,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":92,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":92,"Index":0,"Attempt":0,"Launch Time":1444397343165,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397343390,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":59,"Executor Run Time":152,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586794,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":92,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":301,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"600\",\"name\":\"map\"}","Parent IDs":[300],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":299,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"598\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":300,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"599\",\"name\":\"flatMap\"}","Parent IDs":[299],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397343165,"Completion Time":1444397343390,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":92,"Completion Time":1444397343390,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":93,"Submission Time":1444397343443,"Stage Infos":[{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[93],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":93,"Stage Attempt ID":0,"Task Info":{"Task ID":93,"Index":0,"Attempt":0,"Launch Time":1444397343451,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":93,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":93,"Index":0,"Attempt":0,"Launch Time":1444397343451,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397343526,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":55,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599090,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":93,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":303,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"606\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397343451,"Completion Time":1444397343526,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":93,"Completion Time":1444397343526,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":94,"Submission Time":1444397343585,"Stage Infos":[{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[94],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":94,"Stage Attempt ID":0,"Task Info":{"Task ID":94,"Index":0,"Attempt":0,"Launch Time":1444397343606,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":94,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":94,"Index":0,"Attempt":0,"Launch Time":1444397343606,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397343745,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":29,"Executor Run Time":104,"Result Size":1753,"JVM GC Time":10,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599090,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":94,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":307,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"611\",\"name\":\"filter\"}","Parent IDs":[306],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":305,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"609\",\"name\":\"map\"}","Parent IDs":[304],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":304,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"608\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":306,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"610\",\"name\":\"map\"}","Parent IDs":[305],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397343606,"Completion Time":1444397343746,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":94,"Completion Time":1444397343746,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":95,"Submission Time":1444397343793,"Stage Infos":[{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[95],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":95,"Stage Attempt ID":0,"Task Info":{"Task ID":95,"Index":0,"Attempt":0,"Launch Time":1444397343806,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":95,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":95,"Index":0,"Attempt":0,"Launch Time":1444397343806,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397343861,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":13,"Executor Run Time":36,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599090,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":95,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":311,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"619\",\"name\":\"map\"}","Parent IDs":[310],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":309,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"617\",\"name\":\"map\"}","Parent IDs":[308],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":310,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"618\",\"name\":\"map\"}","Parent IDs":[309],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":308,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"616\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397343806,"Completion Time":1444397343861,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":95,"Completion Time":1444397343861,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":96,"Submission Time":1444397343904,"Stage Infos":[{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[96],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"628\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"628\",\"name\":\"saveAsNewAPIHadoopFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":96,"Stage Attempt ID":0,"Task Info":{"Task ID":96,"Index":0,"Attempt":0,"Launch Time":1444397343919,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":96,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":96,"Index":0,"Attempt":0,"Launch Time":1444397343919,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397344059,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":36,"Executor Run Time":92,"Result Size":1573,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Output Metrics":{"Data Write Method":"Hadoop","Bytes Written":586785,"Records Written":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":96,"Stage Attempt ID":0,"Stage Name":"saveAsNewAPIHadoopFile at SparkAvroExample.scala:173","Number of Tasks":1,"RDD Info":[{"RDD ID":314,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"626\",\"name\":\"map\"}","Parent IDs":[313],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":313,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"625\",\"name\":\"flatMap\"}","Parent IDs":[312],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":312,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"624\",\"name\":\"makeRDD\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:173)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397343918,"Completion Time":1444397344060,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":96,"Completion Time":1444397344060,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":97,"Submission Time":1444397344109,"Stage Infos":[{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[97],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":97,"Stage Attempt ID":0,"Task Info":{"Task ID":97,"Index":0,"Attempt":0,"Launch Time":1444397344126,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":97,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":97,"Index":0,"Attempt":0,"Launch Time":1444397344126,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397344178,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":12,"Executor Run Time":34,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599081,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":97,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:190","Number of Tasks":1,"RDD Info":[{"RDD ID":316,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"632\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:190)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397344125,"Completion Time":1444397344178,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":97,"Completion Time":1444397344178,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":98,"Submission Time":1444397344225,"Stage Infos":[{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[98],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":98,"Stage Attempt ID":0,"Task Info":{"Task ID":98,"Index":0,"Attempt":0,"Launch Time":1444397344247,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":98,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":98,"Index":0,"Attempt":0,"Launch Time":1444397344247,"Executor ID":"0","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397344302,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":10,"Executor Run Time":36,"Result Size":1753,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599081,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":98,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:205","Number of Tasks":1,"RDD Info":[{"RDD ID":320,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"637\",\"name\":\"filter\"}","Parent IDs":[319],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":317,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"634\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":318,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"635\",\"name\":\"map\"}","Parent IDs":[317],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":319,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"636\",\"name\":\"map\"}","Parent IDs":[318],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:205)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397344246,"Completion Time":1444397344302,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":98,"Completion Time":1444397344302,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":99,"Submission Time":1444397344350,"Stage Infos":[{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[99],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":99,"Stage Attempt ID":0,"Task Info":{"Task ID":99,"Index":0,"Attempt":0,"Launch Time":1444397344367,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":99,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":99,"Index":0,"Attempt":0,"Launch Time":1444397344367,"Executor ID":"1","Host":"192.168.2.68","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1444397344425,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.2.68","Executor Deserialize Time":10,"Executor Run Time":42,"Result Size":1754,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":599081,"Records Read":1000}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":99,"Stage Attempt ID":0,"Stage Name":"count at SparkAvroExample.scala:220","Number of Tasks":1,"RDD Info":[{"RDD ID":324,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"645\",\"name\":\"map\"}","Parent IDs":[323],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":322,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"643\",\"name\":\"map\"}","Parent IDs":[321],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":323,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"644\",\"name\":\"map\"}","Parent IDs":[322],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":321,"Name":"/Users/quentin/Dev/avro-serializer/outputrdddfile.bin","Scope":"{\"id\":\"642\",\"name\":\"newAPIHadoopFile\"}","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.count(RDD.scala:1099)\ncom.avroserializer.SparkAvroExample$.usingAvro(SparkAvroExample.scala:220)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample$$anonfun$4.apply(SparkAvroExample.scala:42)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nscala.collection.immutable.List.foreach(List.scala:318)\nscala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nscala.collection.AbstractTraversable.map(Traversable.scala:105)\ncom.avroserializer.SparkAvroExample$.main(SparkAvroExample.scala:42)\ncom.avroserializer.SparkAvroExample.main(SparkAvroExample.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:497)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1444397344366,"Completion Time":1444397344425,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":99,"Completion Time":1444397344425,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1444397344487}
